{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание Глубокой нейронной сети: Шаг за шагом\n",
    "\n",
    "В рамках данной лабораторной работы необходимо будет создать модель на основе глубокой нейронной сети с любым количеством слоев.\n",
    "\n",
    "- В данном notebook, необходимо будет реализовать все функции необходимые для построения глубокой нейронной сети.\n",
    "\n",
    "**В рамках данной лабораторной работы будут приобретены следующие навыки:**\n",
    "- Использование нелинейной функции активации ReLU для улучшения модели\n",
    "- Построение глубокой нейронной сети с более чем 1 скрытым слоем\n",
    "- Реализация простых нейронных сетей\n",
    "\n",
    "**Нотации**:\n",
    "- Надстрочный $[l]$ обозначает число, связанное с $l^{th}$ слоем. \n",
    "    - Например: $a^{[L]}$ это результат вычисления функции активации  $L^{th}$ слоя. $W^{[L]}$ и $b^{[L]}$ являются параметрами $L^{th}$ слоя.\n",
    "- Надстрочный $(i)$ обозначает число, связанное $i^{th}$ примером. \n",
    "    - Например: $x^{(i)}$ это $i^{th}$ обучающий пример.\n",
    "- Подстрочный $i$ обозначает $i^{th}$ нейрон.\n",
    "    - Например: $a^{[l]}_i$ обозначает $i^{th}$ положение нейрона в  $l^{th}$ слое активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Данный материал опирается и использует материалы курса Deep Learning от организации deeplearning.ai`\n",
    " \n",
    " Ссылка на осной курс (для желающих получить сертификаты): https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Пакеты/Библиотеки ##\n",
    "\n",
    "Первоначально необходимо запустить ячейку ниже, чтобы импортировать все пакеты, которые вам понадобятся во время лабораторной работы.\n",
    "- [numpy](www.numpy.org) является основным пакетом для научных вычислений в Python.\n",
    "- [matplotlib](http://matplotlib.org) это пакет для отрисовки графиков в Python.\n",
    "- dnn_utils, testCases дополнительные функции для данного notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Общий план лабораторной работы\n",
    "\n",
    "Чтобы построить нейронную сеть, необходимо реализовать несколько \"вспомогательных функций\". Эти вспомогательные функции будут использованы в следующем задании для построения двухслойной нейронной сети и L-слойной нейронной сети. Вот в общих чертах это задание:\n",
    "\n",
    "- Инициализация параметров для двух-слойной и $L$-слойной нейронной сети.\n",
    "- Реализация модуля прямого распространения.\n",
    "     - Выполните линейную часть шага прямого распространения слоя (в результате получите $Z^{[l]}$).\n",
    "     - Используйте функцию активации (relu/sigmoid).\n",
    "     - Комбинируйте предыдущие два шага в новый [LINEAR->ACTIVATION] прямого распространения.\n",
    "     - Набор [LINEAR->RELU] для L-1 слоев и [LINEAR->SIGMOID] в конце нейронной сети (для конечного слоя $L$).\n",
    "- Вычисление потерь.\n",
    "- Реализация модуля обратного распространения.\n",
    "    - Реализуйте линейную часть для обратного распространения.\n",
    "    - Вычислите градиент функции активации (relu_backward/sigmoid_backward) \n",
    "    - Объедините предыдущие два шага в новую функцию [LINEAR->ACTIVATION].\n",
    "    - Набор [LINEAR->RELU] для L-1 слоев и добавьте [LINEAR->SIGMOID] в функции L_model_backward\n",
    "- Обновите параметры.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Рисунок 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Заметка** Для каждой функции вычисляющей прямое распространение существует соответствующая функция вычисляющая обратное распространение. Именно поэтому на каждом шаге forward модуля необходимо будет хранить некоторые значения в кэше. Кэшированные значения полезны для вычисления градиентов. В модуле обратного распространения необходимо будет использовать кэш для вычисления градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Инициализация\n",
    "\n",
    "Необходимо реализовать две вспомогательные функции, которые будут инициализировать параметры для модели. Первая функция будет использоваться для инициализации параметров двухслойной модели. Во втором случае этот процесс инициализации будет обобщен на $L$ слоёв.\n",
    "\n",
    "### 3.1 - 2-layer Neural Network\n",
    "\n",
    "**Упражнение**: Создайте и инициализируйте параметры 2-слойной нейронной сети.\n",
    "\n",
    "**Инструкции**:\n",
    "- Структура модели: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- Используйте случайную инициализацию для матрицы весов. Используйте `np.random.randn(shape)*0.01` с корректным размером.\n",
    "- Смещения инициализируйте в ноль. Используйте `np.zeros(shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь, содержащий параметры:\n",
    "                    W1 -- матрица весов (n_h, n_x)\n",
    "                    b1 -- вектор смещений (n_h, 1)\n",
    "                    W2 -- матрица весов (n_y, n_h)\n",
    "                    b2 -- вектор смещений (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - L-слойнная Нейронная сеть\n",
    "\n",
    "Инициализация для более глубокой L-слойной нейронной сети является более сложной, поскольку существует гораздо больше матриц весов  и векторов смещения. При выполнении операции `initialize_parameters_deep`, необходимо убедиться, что размеры каждого слоя совпадают. Напоминание о том, что $n^{[l]}$ количество нейроннов в слое $l$. Например если входной матрицы $X$ соответсвует $(12288, 209)$ (с $m=209$ примерами) тогда:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td>Shape of W</td> \n",
    "        <td>Shape of b</td> \n",
    "        <td>Activation</td>\n",
    "        <td>Shape of Activation</td> \n",
    "    <tr> \n",
    "    <tr>\n",
    "        <td> Layer 1 </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> Layer 2 </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "   <tr>\n",
    "        <td> Layer L-1 </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr> \n",
    "   <tr>\n",
    "        <td> Layer L </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "Пример вычисления $W X + b$ в python: \n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{1}  $$\n",
    "\n",
    "где $$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: Реализовать инициалзацию параметров для L-слоёв нейронной сети. \n",
    "\n",
    "**Инструкции**:\n",
    "- Структура нейронной сети *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*.Т.е., каждые $L-1$ слоев используют ReLU функцию активации за которыми следует выходной слой с сигмоидной функцией активации.\n",
    "- Используйте случайную инициализацию для матрицы весов. Используйте `np.random.randn(shape)*0.01` с корректным размером.\n",
    "- Смещения инициализируйте в ноль. Используйте `np.zeros(shape)`.\n",
    "- Необходимо хранить $n^{[l]}$, количество нейронов в каждом слое, в переменной `layer_dims`. Например, в предыдущей лабораторной работы `layer_dims = [2,4,1]`: Где два входных слоя, 4 скрытых и один выходной. `W1` имел размер (4,2), `b1` - (4,1), `W2` - (1,4) и `b2` - (1,1). Сейчас необходимо обобщить на $L$ слоёв! \n",
    "- Вот реализация для $L=1$ (однослойнная нейронная сеть).\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    массив python (список), содержащий размеры каждого слоя в нашей сети \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь, содержащий параметры \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- матрица весов (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- вектор смещений (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Модуль прямого распространения\n",
    "\n",
    "### 4.1 -Линейное распространение\n",
    "Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:\n",
    "Теперь, когда параметры инициализированы, необходимо сделаете модуль прямого распространения. Реализацию необходимо начать с некоторых основных функций, которые будут использоваться позже при реализации модели в следующем порядке:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION с ReLU или Sigmoid функцией активацией. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID\n",
    "\n",
    "Линейное распространение (векторизованный по всем примерам) вычисляется по следующим уравнениям:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{3}$$\n",
    "\n",
    "где $A^{[0]} = X$. \n",
    "\n",
    "**Упражнение**:Реализовать линейную часть прямого распространения.\n",
    "\n",
    "**Напоминание**:\n",
    "Математическое представление одного слоя $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Реализация линейной части прямого распросранения.\n",
    "    \n",
    "    Arguments:\n",
    "    A -- активация с предыдущего слоя (или входные данные): (size of previous layer, number of examples)\n",
    "    W -- матрица весов: numpy массив (size of current layer, size of previous layer)\n",
    "    b -- вектор смещений, numpy массив (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- входные значения функции активации\n",
    "    cache -- a python словарь, содержащий \"A\", \"W\" и \"b\" ; который хранит значения, \n",
    "            необходимые для эффективного вычисления обратного прохода\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 1 line of code)\n",
    "    Z = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[3.1980455  7.85763489]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Функция активации результата прямого вычисления\n",
    "\n",
    "В этой записной книжке используются две функции активации:\n",
    "\n",
    "- **Sigmoid**: Математическая формула $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. В ноутбке уже импортирована функция `sigmoid`. Эта функция возвращает **два** параметра: результат работы функции активации \"`A`\" и \"`cache`\" который содержит \"`Z`\". Для использования данной функции необходимо вызвать код ниже: \n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: Математическая формула $A = RELU(Z) = max(0, Z)$.  В ноутбке уже импортирована функция `relu`. Эта функция возвращает **два** параметра: результат работы функции активации \"`A`\" и \"`cache`\" который содержит \"`Z`\". Для использования данной функции необходимо вызвать код ниже:\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необходимо сгруппировать две функции (линейную и активационную) в одну функцию (LINEAR->ACTIVATION).\n",
    "\n",
    "**Упражнение**: Реализуйте функцию *LINEAR->ACTIVATION* слоя. Математическая интерпретация: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ где активация \"g\" может быть sigmoid() или relu(). Используйте linear_forward() и корректную функцию активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Реализация функцию LINEAR->ACTIVATION слоя\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- активации предыдущего слоя (или входные данные): (size of previous layer, number of examples)\n",
    "    W -- матрица весов: numpy массив (size of current layer, size of previous layer)\n",
    "    b -- вектор смещений, numpy массив (size of the current layer, 1)\n",
    "    activation -- функция активации: \"sigmoid\" или \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- результат вычисления функции активации\n",
    "    cache -- python словарь, который содержит \"linear_cache\" и \"activation_cache\";\n",
    "             хранится для эффективного вычисления обратного прохода\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Вход: \"A_prev, W, b\". Выход: \"A, activation_cache\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        Z, linear_cache = None\n",
    "        A, activation_cache = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Вход: \"A_prev, W, b\". Выход: \"A, activation_cache\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        Z, linear_cache = None\n",
    "        A, activation_cache = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"Sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "       \n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td >[[0.96076066 0.99961336]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td >[[3.1980455  7.85763489]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель с L-слоями \n",
    "\n",
    "Для удобства при реализации нейронной сети $L$ - слоя понадобится функция, которая повторяет предыдущую (`linear_activation_forward` с RELU) $L-1$ раз, а затем следует, что с одним `linear_activation_forward` с SIGMOID.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Рисунок 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* модель</center></caption><br>\n",
    "\n",
    "**Упражнение**: Реализуйте прямое распространение вышеуказанной модели.\n",
    "\n",
    "**Instruction**: В приведенном ниже коде переменная `AL` будет обозначать $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (Это иногда также называют `Yhat`, т.е., это $\\hat{Y}$.) \n",
    "\n",
    "**Подсказка**:\n",
    "- Используйте ранее написанные функции \n",
    "- Используйте цикл for для повторения [LINEAR->RELU] (L-1) раз\n",
    "- Не забывайте добавлять кэш в список \"caches\". Чтобы добавить новое значение `c` к `list`, вы можете использовать `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Реализация прямого распространения для модели [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- данные, numpy массив (input size, number of examples)\n",
    "    parameters -- выход функции initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- последнее значение функции активации\n",
    "    caches -- список caches содержащих:\n",
    "                cache из linear_relu_forward()\n",
    "                cache из linear_sigmoid_forward()\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # количество слоёв в нейронной сети\n",
    "    \n",
    "    # Реализация [LINEAR -> RELU]*(L-1). Добавление \"cache\" к списку \"caches\".\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        A, cache = None\n",
    "\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Реализация LINEAR -> SIGMOID. Добавление \"cache\" к списку \"caches\".\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    AL, cache = None\n",
    "\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Длина caches = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td >[[0.0844367  0.92356858]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list** </td>\n",
    "    <td >2</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализован процесс прямого распространения, который принимает на вход X и и возвращет выход, как вектор $A^{[L]}$ содержащий предсказания. Используйте $A^{[L]}$, вы можете вычислить стоимость ваших прогнозов.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Функция потерь\n",
    "\n",
    "Далее необходимо осуществить прямое и обратное распространение. Вам нужно вычислить стоимость, потому что необходимо проверить, действительно ли модель обучается.\n",
    "\n",
    "**Упражнение**: Реализуйте функцию потерь кросс-энтропии $J$, используя следующую функцию: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{4}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Реализация функции потерь описанную в уравнении (4).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- вектор вероятности, соответствующий предсказаниям (1, number of examples)\n",
    "    Y -- метки (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- результат вычисления функции потерь\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 1 строки кода)\n",
    "    cost = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>**cost**</td>\n",
    "        <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 -Модуль обратного распространения\n",
    "\n",
    "Так же, как и при прямом распространении, необходимо реализовать вспомогательные функции для обратного распространения. Обратное распространение используется для вычисления градиента функции потерь по отношению к параметрам сети.\n",
    "\n",
    "**Напоминание**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Рисунок 3** : *LINEAR->RELU->LINEAR->SIGMOID* <br> *Фиолетовые блоки представляют прямое распространение, а красные блоки-обратное распространение.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "Теперь, подобно прямому распространению, необходимо построить обратное распространение в три этапа:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward где ACTIVATION вычисляется как производная по функции активации либо для ReLU, либо для sigmoid\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Линейное обратное распространение\n",
    "\n",
    "Для слоя $l$, линейная часть выглядит: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$.\n",
    "\n",
    "После вычисления производной $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Необходимо получить $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Рисунок 4** </center></caption>\n",
    "\n",
    "Три выхода $(dW^{[l]}, db^{[l]}, dA^{[l]})$ вычисляются на основе входа $dZ^{[l]}$.Вот необходимые формулы:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{5}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{6}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: Используя формулы выше реализуйте функцию linear_backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Реализовать линейную часть обратного распространения для одного слоя (слой l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- градиент функции потерь по отношению к линейной функции на выходе (of current layer l)\n",
    "    cache -- кортеж значений (A_prev, W, b) исходя из прямого распространения в текущем слое\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- градиент функции потерь по отношению к результату вычисления функции активации (of the previous layer l-1), тот же размер, что и A_prev\n",
    "    dW -- градиент функции потерь по отношению к W (current layer l), тот же размер, что и W\n",
    "    db -- градиент функции потерь по отношению к b (current layer l), тот же размер, что и b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 3 строки кода)\n",
    "    dW = None\n",
    "    db = None\n",
    "    dA_prev = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "#     assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "      <tr>\n",
    "        <td> **dA_prev** </td>\n",
    "        <td > [[ 2.38272385  5.85438014]\n",
    " [ 6.31969219 15.52755701]\n",
    " [-3.97876302 -9.77586689]] </td> \n",
    "      </tr> \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[ 2.77870358 -0.05500058 -5.13144969]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[5.52784019]] </td> \n",
    "    </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Линейная активация для обратного распространения\n",
    "\n",
    "Далее необходимо создать функцию, которая объединяет две вспомогательные функции:\n",
    "**`linear_backward`** и **`linear_activation_backward`**. \n",
    "\n",
    "Для реализации поможет функция `linear_activation_backward`, в ноутбуке загружено две функции, используемые для вычисления обратного распространения:\n",
    "- **`sigmoid_backward`**: Реализует обратное распространение для SIGMOID:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Реализует обратное распространение для RELU:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ функция активации, \n",
    "`sigmoid_backward` и `relu_backward` вычислите $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**Упражнение**: Реализовать обратное распространение для *LINEAR->ACTIVATION* слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        dZ = None\n",
    "        dA_prev, dW, db = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        dZ = None\n",
    "        dA_prev, dW, db = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат with sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.08982777  0.00226265]\n",
    " [ 0.23824996  0.00600122]\n",
    " [-0.14999783 -0.00377826]] </td> \n",
    "\n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[-0.06001514 -0.09687383 -0.10598695]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[0.06180098]] </td> \n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат with relu**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 2.38272385  5.85438014]\n",
    " [ 6.31969219 15.52755701]\n",
    " [-3.97876302 -9.77586689]] </td> \n",
    "\n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 2.77870358 -0.05500058 -5.13144969]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[5.52784019]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-модель обратного распространения\n",
    "\n",
    "Теперь необходимо реализовать обратную функцию для нейронной сети. Напомним, что при реализации функции `L_model_forward` на каждой итерации вы сохраняли cache, содержащий (X,W,b и z). В модуле обратного распространения используются эти переменные для вычисления градиентов. Поэтому в функции `L_model_backward` перебираются все скрытые слои в обратном порядке, начиная со слоя $L$. На каждом шаге необходимо использовать кэшированные значения для слоя $l$ для обратного распространения через слой $l$. На рисунке 5 ниже показан обратный проход.\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Рисунок 5**  </center></caption>\n",
    "\n",
    "**Инициализация параметров**:\n",
    "Код должен вычислять `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "Для этого используйте эту формулу:\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # производная от функции потерь по отношению ко всем параметрам\n",
    "```\n",
    "\n",
    "Далее необходимо использовать градиент `dAL` для дальнейшего обратного движения по нейронной сети. Как изображено на Рисунке 5, параметр `dAL` передается в LINEAR->SIGMOID в функцию обратного распространения. После этого необходимо использовать цикл `for`  чтобы перебрать все остальные слои, используя LINEAR->RELU функцию. Вы должны хранить каждый из них dA, dW, и db в словаре градиентов. Для этого используется следующую формула: \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "Например, для $l=3$ это будет хранить $dW^{[l]}$ в `grads[\"dW3\"]`.\n",
    "\n",
    "**Упражнение**: Реализуйте обратное распространение ошибки *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространения для [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- вектор вероятностей, выход прямого прохождения (L_model_forward())\n",
    "    Y -- вектор меток\n",
    "    caches -- список caches содержащий:\n",
    "                cache из linear_activation_forward() с \"relu\"\n",
    "                cache из linear_activation_forward() с \"sigmoid\"\n",
    "    \n",
    "    Returns:\n",
    "    grads -- словарь градиентов\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # количество слоёв\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (1 строка кода)\n",
    "    dAL =  None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # L слой (SIGMOID -> LINEAR). Вход: \"AL, Y, caches\". Выход: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (approx. 2 lines)\n",
    "    current_cache = None\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # l-слой: (RELU -> LINEAR) градиенты.\n",
    "        # Вход: \"grads[\"dA\" + str(l + 2)], caches\". Выход: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        current_cache = None\n",
    "        dA_prev_temp, dW_temp, db_temp = None\n",
    "        grads[\"dA\" + str(l + 1)] = None\n",
    "        grads[\"dW\" + str(l + 1)] = None\n",
    "        grads[\"db\" + str(l + 1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, AL, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[-0.09686122 -0.04840482 -0.11864308]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.262595]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[-0.71011462 -0.22925516]\n",
    " [-0.17330152 -0.05594909]\n",
    " [-0.03831107 -0.01236844]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Обновление параметров\n",
    "\n",
    "В части необходимо обновить параметры модели, используя градиентный спуск: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "где $\\alpha$ скорость градиентного спуска. После вычисления обновленных параметров сохраните их в словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: Реализуйте `update_parameters()` для обновления параметров с использованием градиентного спуска.\n",
    "\n",
    "**Инструкции**:\n",
    "Обновление параметров с помощью градиентного спуска для каждого $W^{[l]}$ и $b^{[l]}$ для всех слоёв $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Обновление параметров с использованием градиентного спуска\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры \n",
    "    grads -- python словарь содержащий градиенты, выход функции L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь, содержащий ваши обновленные параметры \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # количество слоев в нейроной сети\n",
    "\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 3 строки кода)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "   <tr>\n",
    "    <td> W1 </td> \n",
    "    <td> [[ 1.72555789  0.3700272   0.07818896]\n",
    " [-1.8634927  -0.2773882  -0.35475898]\n",
    " [-0.08274148 -0.62700068 -0.04381817]\n",
    " [-0.47721803 -1.31386475  0.88462238]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td> b1 </td> \n",
    "    <td> [[-0.07593768]\n",
    " [-0.07593768]\n",
    " [-0.07593768]\n",
    " [-0.07593768]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td> W2 </td> \n",
    "    <td> [[ 0.71838378  1.70957306  0.05003364 -0.40467741]\n",
    " [-0.54535995 -1.54647732  0.98236743 -1.10106763]\n",
    " [-1.18504653 -0.2056499   1.48614836  0.23671627]]</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td> b2 </td> \n",
    "    <td> [[-0.08616376]\n",
    " [-0.08616376]\n",
    " [-0.08616376]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Используемый материал:\n",
    "- Курс Deep Learning; https://www.coursera.org/specializations/deep-learning"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
