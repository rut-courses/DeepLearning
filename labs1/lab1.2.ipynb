{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация на тестовых данных с помощью нейронной сети с одним скрытым слоем\n",
    "\n",
    "В рамках данной лабораторной работы необходимо будет создать модель нейронной сети с одним скрытым слоем.\n",
    "\n",
    "**В рамках данной лабораторной работы будут приобретены следующие навыки (знания):**\n",
    "- Реализация нейронной сети с одним скрытым слоем для задачи классификации на 2 класса;\n",
    "- Использование блоков с нелинейной функцией активации - tanh;\n",
    "- Вычисление функции потерь кросс-энтропии (cross entropy loss);\n",
    "- Реализация вычислений прямого и обратного распространения для нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Данный материал опирается и использует материалы курса Deep Learning от организации deeplearning.ai`\n",
    " \n",
    " Ссылка на основной курс (для желающих получить дополнительный сертификаты): https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Пакеты/Библиотеки\n",
    "\n",
    "Первоначально необходимо запустить ячейку ниже, чтобы импортировать все пакеты, которые вам понадобятся во время лабораторной работы.\n",
    "- [numpy](www.numpy.org) является основным пакетом для научных вычислений в Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) предоставляет простые и эффективные инструменты для построения моделей и анализа данных.\n",
    "- [matplotlib](http://matplotlib.org) это пакет для отрисовки графиков в Python.\n",
    "- `planar_utils` предоставляет различные полезные функции, используемые в данной лабораторной работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # для того чтобы результаты были похожи на ответы необходимо сделать так, чтобы случайности были предсказуемы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Датасет\n",
    "\n",
    "Во-первых, давайте загрузим набор данных, с которым будем работать.\n",
    "Следующий код загрузит набор данных из 2-х классов в переменные `X` and `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_planar_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем датасет с использованием matplotlib.\n",
    "Данные выглядят как \"цветок\" с красными (`y=0`) и синими (`y=1`) точками. Ваша цель построить модель, обучив её на этом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[0, :], X[1, :], c=Y[0], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На входе:\n",
    "    - numpy-array (матрица) X, который содержит признаки (x1, x2)\n",
    "    - numpy-array (вектор) Y, который содержит метки (red:0, blue:1).\n",
    "\n",
    "Давайте сначала лучше поймем наши данные.\n",
    "\n",
    "**Упражнение** Как много примеров в обучающей выборке? Какой размер (`shape`) у переменных `X` и `Y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 3 строки кода)\n",
    "shape_X = None\n",
    "shape_Y = None\n",
    "m = None  # размер обучающего датасета\n",
    "### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "print ('Размер X: ' + str(shape_X))\n",
    "print ('Размер Y: ' + str(shape_Y))\n",
    "print ('В выборке m = %d обучающих примеров!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "       \n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**shape of X**</td>\n",
    "    <td> (2, 400) </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**shape of Y**</td>\n",
    "    <td>(1, 400) </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**m**</td>\n",
    "    <td> 400 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Логистическая регрессия\n",
    "\n",
    "Прежде чем построить полную нейронную сеть, давайте сначала посмотрим, как работает логистическая регрессия в этой задаче.\n",
    "Для этого можно использовать встроенные функции sklearn.\n",
    "Выполните приведенный ниже код для создания классификатора на основе логистической регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.linear_model.LogisticRegressionCV()\n",
    "clf.fit(X.T, Y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы можете построить границу принятия решения для этих моделей. Выполните приведенный ниже код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Построение границы принятия решения для логистической регрессии\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X, Y[0])\n",
    "plt.title(\"Logistic Regression\")\n",
    "\n",
    "# Вывод точности (accuracy) предсказания\n",
    "LR_predictions = clf.predict(X.T)\n",
    "print ('Accuracy для логистической регрессии: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +\n",
    "       '% ' + \"(процент правильно размечанных алгоритмом точек данных)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 47% </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерпретация**: Набор данных не является линейно разделяемым, поэтому логистическая регрессия работает не достаточно хорошо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Модель нейронной сети\n",
    "\n",
    "Логистическая регрессия плохо работала на \"цветочном\" наборе данных. Далее вы будете обучать нейронную сеть с одним скрытым слоем.\n",
    "\n",
    "**Вот наша модель**:\n",
    "<img src=\"images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "**Математическое объяснение алгоритма**:\n",
    "\n",
    "Для одного примера $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Учитывая прогнозы на всех примерах, вы также можете вычислить стоимость $J$ следующим образом: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Напоминание** Общая методология построения нейронной сети заключается в следующем:\n",
    "    1. Определение структуры нейронной сети (`# количество нейронов на входе`,  `# количество нейронов в скрытом слое`, и т.п.).\n",
    "    2. Инициализация параметров модели\n",
    "    3. Цикл:\n",
    "        - Прямой проход по нейронной сети\n",
    "        - Вычисление потерь\n",
    "        - Обратное распространение для получения градиентов \n",
    "        - Обновление параметров (градиентный спуск)\n",
    "\n",
    "Вы часто создаете вспомогательные функции для вычисления шагов 1-3, а затем объединяете их в одну функцию, которую называется в нашем курсе `nn_model()`. Как только вы построили `nn_model()`, подобрав параметры, вы можете делать прогнозы по новым данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Определение структуры нейронной сети\n",
    "\n",
    "**Упражнение** Определить три параметра:\n",
    "    - n_x: размер входного слоя\n",
    "    - n_h: размер скрытого слоя (по дефолту установим в 4) \n",
    "    - n_y: размер выходного слоя\n",
    "\n",
    "**Подсказка**: Используйте размеры X и Y для получения n_x и n_y.\n",
    "Также, размер скрытого слоя должен быть равен 4 (вы можете менять данное значение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: layer_sizes\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- размер входного датасета (input size, number of examples)\n",
    "    Y -- размер матрицы с метками (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \"\"\"\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 3 строки кода)\n",
    "    n_x = None\n",
    "    n_h = None\n",
    "    n_y = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n_x, n_h, n_y) = layer_sizes(X, Y)\n",
    "print(\"n_x = \" + str(n_x))\n",
    "print(\"n_h = \" + str(n_h))\n",
    "print(\"n_y = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**n_x**</td>\n",
    "    <td> 2 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**n_h**</td>\n",
    "    <td> 4 </td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>**n_y**</td>\n",
    "    <td> 1 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Инициализация параметров модели ####\n",
    "\n",
    "**Упражнение** Реализовать функцию `initialize_parameters()`.\n",
    "\n",
    "**Инструкции**:\n",
    "- Убедитесь в правильности размеров ваших параметров. При необходимости обратитесь к рисунку нейронной сети выше.\n",
    "- Необходимо инициализировать матрицы весов случайными значениями.  Используйте: `np.random.randn(a,b) * 0.01` для инициализации матрицы со случайными значениями размером (a,b).\n",
    "- Необходимо инициализировать векторы смещения как нули. Используйте: `np.zeros((a,b))` для инициализации матрицы с нулевыми значениями размером (a,b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- размер входного слоя\n",
    "    n_h -- размер скрытого слоя\n",
    "    n_y -- размер выходного слоя\n",
    "    \n",
    "    Returns:\n",
    "    params -- python словарь, содержащий следующие значения:\n",
    "                    W1 -- матрица весов скрытого слоя размером (n_h, n_x)\n",
    "                    b1 -- вектор смещений скрытого слоя размером (n_h, 1)\n",
    "                    W2 -- матрица весов выходного слоя размером (n_y, n_h)\n",
    "                    b2 -- вектор смещений выходного слоя размером (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00416758 -0.00056267]\n",
    " [-0.02136196  0.01640271]\n",
    " [-0.01793436 -0.00841747]\n",
    " [ 0.00502881 -0.01245288]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Цикл\n",
    "\n",
    "**Упражнение** Реализуйте `forward_propagation()`.\n",
    "\n",
    "**Инструкция**:\n",
    "- Посмотрите выше на математическое представление классификатора.\n",
    "- Вы можете использовать функцию `sigmoid()`. Она встроена в ноутбук.\n",
    "- Вы можете использовать гиперболический тангенс `np.tanh()`. Это часть numpy библиотеки.\n",
    "- Вы должны выполнить следующие шаги:\n",
    "    1. Извлеките каждый параметр из словаря \"parameters\" (который является выводом `initialize_parameters()`) с использованием `parameters[\"..\"]`.\n",
    "    2. Реализуйте прямое распространение. Вычислите $Z^{[1]}, A^{[1]}, Z^{[2]}$ и $A^{[2]}$ (вектор всех ваших прогнозов на всех примерах в обучающем наборе).\n",
    "- Значения, необходимые для обратного распространения, сохранить в \"`cache`\". `cache` будет передан в качестве входных данных для функции обратного распространения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- размер матрицы признаков (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- результат вычисления sigmoid с выходного слоя\n",
    "    cache -- словарь с параметрами \"Z1\", \"A1\", \"Z2\" и \"A2\"\n",
    "    \"\"\"\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Реализуйте прямое распространение вычислив значение A2\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(X, parameters)\n",
    "\n",
    "# Примечание: используется среднее значение здесь только для того, чтобы убедиться, что ваш вывод совпадает с ответом. \n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <td> 0.0003302739 0.000329736 -0.000043569 0.4999891 0.500109</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда вы вычислили $A^{[2]}$, который содержит $a^{[2](i)}$ для каждого примера, вы можете вычислить функцию затрат следующим образом:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{7}$$\n",
    "\n",
    "**Упражнение**: Реализуйте `compute_cost()` вычислив функцию потерь $J$.\n",
    "\n",
    "**Инструкция**:\n",
    "- Существует много способов реализовать кросс-энтропийную функцию потерь. Ниже приведён пример реализации\n",
    "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs) # цикл нельзя использовать!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: compute_cost\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Вычисляет функцию потерь перекрестной энтропии, приведенную в уравнении (7)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- выход из выходного слоя нейронной сети (1, number of examples)\n",
    "    Y -- \"true\" вектор меток (1, number of examples)\n",
    "    parameters -- python словарь с параметрами W1, b1, W2 и b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy результат вычисления функции потерь (7)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    logprobs = None\n",
    "    cost = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    cost = np.squeeze(cost) \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cost = \" + str(compute_cost(A2, Y, parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td>**cost**</td>\n",
    "    <td> 0.6930480201239823 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя кэш, вычисленный во время прямого распространения, теперь можно реализовать обратное распространение.\n",
    "\n",
    "**Упражнение** Реализуйте функцию `backward_propagation()`.\n",
    "\n",
    "**Инструкция**:\n",
    "Обратное распространение обычно является одной из самых трудных частей глубокого обучения. Вам в помощь ниже представлен краткий обзор используемых математических формул.\n",
    "\n",
    "<img src=\"images/grad_summary.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "<!--\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } = \\frac{1}{m} (a^{[2](i)} - y^{(i)})$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_2 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } a^{[1] (i) T} $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial b_2 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)}}}$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} } =  W_2^T \\frac{\\partial \\mathcal{J} }{ \\partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} }{ \\partial W_1 } = \\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)} }  X^T $\n",
    "\n",
    "$\\frac{\\partial \\mathcal{J} _i }{ \\partial b_1 } = \\sum_i{\\frac{\\partial \\mathcal{J} }{ \\partial z_{1}^{(i)}}}$\n",
    "\n",
    "- Note that $*$ denotes elementwise multiplication.\n",
    "- The notation you will use is common in deep learning coding:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "    \n",
    "\n",
    "\n",
    "- Подсказка:\n",
    "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`. !-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространения ошибки\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь с параметрами \n",
    "    cache -- словарь, содержащий \"Z1\", \"A1\", \"Z2\" и \"A2\".\n",
    "    X -- матрица признаков (2, number of examples)\n",
    "    Y -- вектор меток (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python словарь, содержащий градиенты относительно различных параметров\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Получение W1 и W2 из словаря \"parameters\".\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    W1 = None\n",
    "    W2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "    # Получение A1 и A2 из словаря \"cache\".\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    A1 = None\n",
    "    A2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Обратное распространение: вычисление dW1, db1, dW2, db2. \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 6 строк кода)\n",
    "    dZ2 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    dZ1 = None\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward_propagation(parameters, cache, X, Y)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>**dW1**</td>\n",
    "    <td> [[ 0.00023606 -0.00207914]\n",
    " [ 0.0002091  -0.00178201]\n",
    " [-0.00012051  0.0010843 ]\n",
    " [-0.00051496  0.00449162]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**db1**</td>\n",
    "    <td>  [[ 1.06848030e-07]\n",
    " [-9.70907252e-07]\n",
    " [-7.20012658e-08]\n",
    " [ 3.03048452e-07]] </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**dW2**</td>\n",
    "    <td> [[-1.75478714e-05  3.70240274e-03 -1.25686736e-03 -2.55725650e-03]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**db2**</td>\n",
    "    <td> [[-1.0892314e-05]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение** Реализуйте градиентный спуск. Необходимо использовать (dW1, db1, dW2, db2) для того чтобы обновить (W1, b1, W2, b2).\n",
    "\n",
    "**Общее правило градиентного спуска**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ где $\\alpha$ скорость градиентного спуска и $\\theta$ оптимизируемый параметр.\n",
    "\n",
    "**Иллюстрация**: Алгоритм градиентного спуска с хорошей скоростью обучения (сходящийся) и плохой скоростью обучения (расходящийся). Автор изображений Адам Харли.\n",
    "\n",
    "<img src=\"images/sgd.gif\" style=\"width:400;height:400;\"> <img src=\"images/sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Обновление параметров с использованием градиентного спуска\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь с параметрами \n",
    "    grads -- python словарь, содержащий градиенты относительно различных параметров\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь с обновлёнными параметрами \n",
    "    \"\"\"\n",
    "    # Получение всех параметров из словаря \"parameters\".\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Получение всех параметров из словаря \"grads\"\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    ## ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Правило обновления параметров\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[-0.00445085  0.0019323 ]\n",
    " [-0.02161288  0.01854112]\n",
    " [-0.01778975 -0.00971864]\n",
    " [ 0.00564676 -0.01784282]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[-1.28217636e-07]\n",
    " [ 1.16508870e-06]\n",
    " [ 8.64015190e-08]\n",
    " [-3.63658142e-07]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-0.01055846 -0.01353296  0.00702278  0.02599079]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[1.30707768e-05]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Объединение частей 4.1, 4.2 и 4.3 в nn_model()\n",
    "\n",
    "**Упражнение** Постройте нейросетевую модель в `nn_model()`.\n",
    "\n",
    "**Инструкция**: Нейросетевая модель должна использовать предыдущие функции в правильном порядке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- матрица признаков (2, number of examples)\n",
    "    Y -- вектор меток (1, number of examples)\n",
    "    n_h -- размер скрытого слоя\n",
    "    num_iterations -- количество итераций градиентного спуска\n",
    "    print_cost -- если True, каждый 1000-й шаг выводится значение функции потерь\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- обученные параметры модели.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 5 строки кода)\n",
    "    parameters = None\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 4 строки кода)\n",
    "        # Прямое распространение. Вход: \"X, parameters\". Выход: \"A2, cache\".\n",
    "        A2, cache = None\n",
    "        \n",
    "        # Функция потерь. Вход: \"A2, Y, parameters\". Выход: \"cost\".\n",
    "        cost = None\n",
    " \n",
    "        # Backpropagation. Вход: \"parameters, cache, X, Y\". Выход: \"grads\".\n",
    "        grads = None\n",
    "        \n",
    "        # Градиентный спуск. Вход: \"parameters, grads\". Выход: \"parameters\".\n",
    "        parameters = None\n",
    "        \n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn_model(X, Y, 4, num_iterations=10000, print_cost=False)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td>**W1**</td>\n",
    "    <td> [[  0.1444612   -9.68515273]\n",
    " [-11.13256001   3.33888681]\n",
    " [-11.46577211 -13.41879812]\n",
    " [  9.2555948  -10.19785466]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1**</td>\n",
    "    <td> [[ 0.0158193 ]\n",
    " [-0.40866061]\n",
    " [-0.06494445]\n",
    " [ 0.01700459]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[-11.98559834   3.44419425   6.1513706   10.5212533 ]] </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  <tr>\n",
    "    <td>**b2**</td>\n",
    "    <td> [[-0.06602615]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Предсказания\n",
    "\n",
    "**Упражнение** Используйте свою модель для прогнозирования. Используйте прямое распространение для прогнозирования результатов.\n",
    "\n",
    "**Напоминание**: predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: predict\n",
    "\n",
    "def predict(parameters, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Используя обученные параметры, предсказывает класс для каждого примера в X\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- обученные параметры модели.\n",
    "    X -- матрица признаков (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- вектор предсказаний модели (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    A2, cache = None\n",
    "    predictions = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(parameters, X)\n",
    "print(\"predictions mean = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**predictions mean**</td>\n",
    "    <td> 0.4875 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните следующий код для тестирования модели с одним скрытым слоем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Построим модель с n_h нейронов в скрытом слое\n",
    "parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Нарисуем границы\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0])\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td>**Cost after iteration 9000**</td>\n",
    "    <td> 0.218558 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "  <tr>\n",
    "    <td>**Accuracy**</td>\n",
    "    <td> 90% </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность действительно высока по сравнению с логистической регрессией. \n",
    "Модель выучила узоры листьев цветка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Настройка размера скрытого слоя\n",
    "\n",
    "Выполните следующий код. Это может занять 1-2 минуты. Вы будете наблюдать различное поведение модели для различных размеров скрытого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Hidden Layer of size %d' % n_h)\n",
    "    parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0])\n",
    "    predictions = predict(parameters, X)\n",
    "    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "    print (\"Accuracy для {} скрытых нейронов: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерпретация**:\n",
    "- Большие модели (с большим количеством скрытых нейронов) способны лучше соответствовать тренировочному набору, пока в конечном итоге самые большие модели не будут соответствовать данным.\n",
    "- Лучший размер скрытого слоя, находится в районе n_h = 5. Действительно, значение здесь хорошо вписывается в данные, не вызывая также заметного переобучения.\n",
    "- Позже вы также узнаете о регуляризации, которая позволяет использовать очень большие модели (например, n_h = 50) без большого переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Примеры на других наборах данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Датасеты\n",
    "noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
    "\n",
    "datasets = {\"noisy_circles\": noisy_circles,\n",
    "            \"noisy_moons\": noisy_moons,\n",
    "            \"blobs\": blobs,\n",
    "            \"gaussian_quantiles\": gaussian_quantiles}\n",
    "\n",
    "### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (выберите датасет)\n",
    "dataset = \"noisy_moons\"\n",
    "### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "X, Y = datasets[dataset]\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "# make blobs binary\n",
    "if dataset == \"blobs\":\n",
    "    Y = Y % 2\n",
    "\n",
    "# Visualize the data\n",
    "plt.scatter(X[0, :], X[1, :], c=Y[0], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_h = 50\n",
    "plt.title('Hidden Layer of size %d' % n_h)\n",
    "parameters = nn_model(X, Y, n_h, num_iterations = 5000)\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0])\n",
    "predictions = predict(parameters, X)\n",
    "accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)\n",
    "print (\"Accuracy для {} скрытых нейронов: {} %\".format(n_h, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемый материал:\n",
    "- Курс Deep Learning; https://www.coursera.org/specializations/deep-learning\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}