{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка корректности работы градиентного спуска\n",
    "\n",
    "Ваша задача в компании это построить модель на основе методов глубокого обучения по детектированию аномалий в платежах совершаемых пользователями с использованием мобильных устройств, поскольку некоторые платежы могут совершаться мошенниками или хакерами.\n",
    "\n",
    "Но обратное распространение ошибки довольно сложно реализовать и иногда, при реализации, алгоритм имеет ошибки. Ваш руководитель хочет, чтобы вы реализовали метод проверки корректности работы обратного распространения ошибки. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`Данный материал опирается и использует материалы курса Deep Learning от организации deeplearning.ai`\n",
    " \n",
    " Ссылка на осной курс (для желающих получить сертификаты): https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 - Пакеты/Библиотеки ##\n",
    "\n",
    "Первоначально необходимо запустить ячейку ниже, чтобы импортировать все пакеты, которые вам понадобятся во время лабораторной работы.\n",
    "- [numpy](www.numpy.org) является основным пакетом для научных вычислений в Python.\n",
    "- gc_utils предоставляет различные полезные функции, используемые в данной лабораторной работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCases import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Как проверить работы градиентного спуска?\n",
    "\n",
    "В алгоритме обратного распространения ошибки вычисляются градиенты  $\\frac{\\partial J}{\\partial \\theta}$, где $\\theta$ соответствует параметрам модели. $J$ вычислялось с использованием прямого распросранения и и функции потерь.\n",
    "\n",
    "Поскольку прямое распространение и функцию потерь относительно легко реализовать, то можно быть увереным практически на 100% что вычисление $J$ корректно. В связи с этом можно использовать вычисленное значение $J$ для проверки и вычисления $\\frac{\\partial J}{\\partial \\theta}$. \n",
    "\n",
    "Определение производной (или градиентов):\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "В рамках данной лабораторной работы вы узнаете как:\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ - убедиться в том, что это выражение вычисляется корректно. \n",
    "- Вычислить $J(\\theta + \\varepsilon)$ и $J(\\theta - \\varepsilon)$ (при условии что $\\theta$ действительное число), при условии уверенности в том, что верно вычисленно $J$. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Проверка градиента в одномерном измерении\n",
    "\n",
    "Рассмотрим одномерную линейную функцию $J(\\theta) = \\theta x$. Модель содержит единственный параметр $\\theta$, и принимает на вход $x$.\n",
    "\n",
    "Необходимо вычислить  $J(.)$ и её производную $\\frac{\\partial J}{\\partial \\theta}$. Затем необходимо использовать проверку градиента для того, чтобы убедится в корректности расчёта $J$. \n",
    "\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> <u> **Figure 1** </u>: **1D linear model**<br> </center></caption>\n",
    "\n",
    "На диаграмме выше показано ключевые шаги вычисления: первый шаг начинается с $x$, затем вычисляется функция функция потерь. Затем вычисляется производная $\\frac{\\partial J}{\\partial \\theta}$ (\"backward propagation\"). \n",
    "\n",
    "**Упражнение**: Реализовать \"прямое распространение\" и \"обратное распространение\" для этой простой функции. Т.е., вычислить и $J(.)$ и её производную по $\\theta$, с использованием двух раздельных функций. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: forward_propagation\n",
    "\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Реализация функции линейного прямого распространения (вычисление потерь) (J(theta) = theta * x)\n",
    "    \n",
    "    Arguments:\n",
    "    x -- действительное число\n",
    "    theta -- параметр thetha, также число\n",
    "    \n",
    "    Returns:\n",
    "    J -- значение функции потерь J, вычисленное с использованием формулы J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (прибл. 1 строка кода)\n",
    "    J = np.dot(theta, x)\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "J = 8\n"
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=>\n",
    "    <tr>\n",
    "        <td>  ** J **  </td>\n",
    "        <td> 8 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Упражнение**: Реализовать обратное распространение. Для этого вычислить производную $J(\\theta) = \\theta x$ по $\\theta$. Сохранить вычисления в переменную $dtheta = \\frac { \\partial J }{ \\partial \\theta} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: backward_propagation\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Вычисление производной по функции J по параметру thets.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- действительное число\n",
    "    theta -- параметр thetha, также число\n",
    "    \n",
    "    Returns:\n",
    "    dtheta -- градиент по theta\n",
    "    \"\"\"\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (прибл. 1 строка кода)\n",
    "    dtheta = x\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "dtheta = 2\n"
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  ** dtheta **  </td>\n",
    "        <td> 2 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Упражнение**: Показать, что `backward_propagation()` функция корректно вычисляет градиент $\\frac{\\partial J}{\\partial \\theta}$.\n",
    "\n",
    "**Инструкция**:\n",
    "- Вычислите \"gradapprox\" с использованием формулы (1) и небольшого числа $\\varepsilon$. Следующие шаги:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- Затем вычислите градиент с использованием обратного распространения и сохраните результаты в переменную \"grad\"\n",
    "- В итоге вычислите относительное различие между \"gradapprox\" и \"grad\" с использованием следующей формулы:\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "3 Steps для вычисления этой формулы\n",
    "   - 1'. вычислить числитель с использованием np.linalg.norm(...)\n",
    "   - 2'. вычислить знаменатель с использованием np.linalg.norm(...).\n",
    "   - 3'. разделить их.\n",
    "- Если различие небольшое (скажем $10^{-7}$), то можно с большой долей уверенности утверждать что градиент вычислен корректно. С другой стороны, большое различие показывает, что в вычислении обратного распространения закралась ошибка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: gradient_check\n",
    "\n",
    "def gradient_check(x, theta, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространения.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- действительное число\n",
    "    theta -- параметр thetha, также число\n",
    "    epsilon -- небольшой сдвиг между координатами\n",
    "    \n",
    "    Returns:\n",
    "    difference -- различие между апроксимацией градиента и обратным распространением\n",
    "    \"\"\"\n",
    "    # Вычислите gradapprox\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (прибл. 5 строк кода)\n",
    "    thetaplus = theta + epsilon         \n",
    "    thetaminus = theta - epsilon                              \n",
    "    J_plus = np.dot(thetaplus, x)                                 \n",
    "    J_minus = np.dot(thetaminus, x)\n",
    "    gradapprox = (J_plus - J_minus)/(2*epsilon)\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Вычислите grad\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (прибл. 1 строка кода)\n",
    "    grad = x\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "    # Проверьте gradapprox\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (прибл. 1-3 строки кода)\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator/denominator\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    if difference < 1e-7:\n",
    "        print (\"Вычисление градиента корректено!\")\n",
    "    else:\n",
    "        print (\"В вычислении градиента ошибка!\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Вычисление градиента корректено!\ndifference = 2.919335883291695e-10\n"
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Ожидаемый результат**:\n",
    "Вычисление градиента корректено!\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  ** difference **  </td>\n",
    "        <td> 2.919335883291695e-10 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "При обучении нейронной сети обычно $\\theta$ содержит несколько матриц $W^{[l]}$ и векторов смещений $b^{[l]}$. В следующей части лабораторной работы необходимо реализовать проверку работы обратного распространения для N-слойной нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Проверка градиента в N-мерном измерении"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "На следующем изображении представлена диаграмма описывающая прямое и обратное распространение для модели детектирования аномалий.\n",
    "\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **Figure 2** </u>: Глубокая нейронная сеть<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Реализация прямого распространения как на Рисунке 3.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- обучающий набор примеров m\n",
    "    Y -- метки для m примеров \n",
    "    parameters -- python словарь, содержащий параметры \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- матрица весов 1 слоя (5, 4)\n",
    "                    b1 -- вектор отклонений 1 слоя (5, 1)\n",
    "                    W2 -- матрица весов 2 слоя (3, 5)\n",
    "                    b2 -- вектор отклонений 2 слоя (3, 1)\n",
    "                    W3 -- матрица весов 3 слоя (1, 3)\n",
    "                    b3 -- вектор отклонений 3 слоя (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- потери\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Потери\n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространения как на Рисунке 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- входные данные (input size, 1)\n",
    "    Y -- метка\n",
    "    cache -- кэш входных данных forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- словарь градиентов для каждой переменной.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) \n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Допустим вы получили некоторые результат работы вашей модели на тестовой выборке, но вы не до конца уверены в корректности работы данной модели. Давайте проверим работу обратного распространения ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Как в 1) и 2), необходимо сравнить \"gradapprox\" с градиентом вычисленным по формуле обратного распросранения ошибки:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Тем не менее, $\\theta$ уже не скаляр. Это словарь \"parameters\". Для вас реализована функция \"`dictionary_to_vector ()`. Она преобразует словарь \"parameters\" в Вектор под названием \"values\", полученный путем преобразования всех параметров (W1, b1, W2, b2, W3, b3) в векторы и объединения их.\n",
    "\n",
    "Обратная функция \"`vector_to_dictionary`\" которая делает обратное преобразование в словарь \"parameters\".\n",
    "\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **Figure 2** </u>: **dictionary_to_vector() and vector_to_dictionary()**<br> You will need these functions in gradient_check_n()</center></caption>\n",
    "\n",
    "Также в лабораторной работе для вас уже преобразовали словарь \"gradients\" в вектор \"grad\" с помощью функции gradients_to_vector().\n",
    "\n",
    "**Упражнение**: Реализовать gradient_check_n().\n",
    "\n",
    "**Инструкция**: Вот псевдокод, который поможет вам реализовать проверку градиента.\n",
    "\n",
    "For each i in num_parameters:\n",
    "- Вычислите `J_plus[i]`:\n",
    "    1. Набор $\\theta^{+}$ скопировать в `np.copy(parameters_values)`\n",
    "    2. В набор $\\theta^{+}_i$ добавить $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Вычислить $J^{+}_i$ с использованием `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- Вычислить `J_minus[i]`: делать точно такие же шаги, только для $\\theta^{-}$\n",
    "- Вычислить $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Таким образом, вы получите вектор gradapprox, где gradapprox[i]  аппроксимация градиента относительно `parameter_values[i]`. Теперь вы можете сравнить вектор gradapprox с вектором градиентов из обратного распространения. Вычислите:\n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Проверка вычисления градиента backward_propagation_n по forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- выход backward_propagation_n, содержит градиенты по всем параметрам. \n",
    "    x -- взодная матрица признаков (input size, 1)\n",
    "    y -- метка \n",
    "    epsilon -- небольшое значение сдвига, используемое в формуле (1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- разиличие между градиентами, расчитанное по формуле (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Настройка переменных\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Вычисление gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        # Вычисление J_plus[i]. Вход: \"parameters_values, epsilon\". Выход = \"J_plus[i]\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(parameters_values)                                        # Шаг 1\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon                                   # Шаг 2\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))   # Шаг 3\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "        # Вычисление J_minus[i]. Вход: \"parameters_values, epsilon\". Выход = \"J_minus[i]\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(parameters_values)                                       # Шаг 1\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon                                 # Шаг 2        \n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Шаг 3\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "        # Вычисление gradapprox[i]\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (approx. 1 line)\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i])/(2*epsilon)\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    # Сравнение gradapprox с обратным распространением.\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (approx. 1 line)\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                                           # Step 1'\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2'\n",
    "    difference = numerator/denominator                                        # Step 3'\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "    if difference > 1e-7:\n",
    "        print (\"\\033[91m\" + \"Ошибка в вычисление обратного распространения! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Обратное распространение ошибки работает корректно! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\u001b[91mОшибка в вычисление обратного распространения! difference = 1.1890913023330276e-07\u001b[0m\n"
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  ** Ошибка в вычисление обратного распространения!**  </td>\n",
    "        <td> difference = 1.1890913023330276e-07 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Похоже, что в коде backward_propagation_n, который вам был предоставлен, были ошибки! Хорошо, что вы реализовали проверку градиента. Вернитесь к \"backward_propagation\" и попробуйте найти/исправить ошибки *(подсказка: проверьте dW2 и db1)*. Повторите проверку градиента, когда вы решите, что исправили весь код. Помните, что вам нужно будет повторно выполнить ячейку, определяющую `backward_propagation_n ()`, если вы измените код.\n",
    "\n",
    "**Заметка** \n",
    "- Проверка градиента работает медленно! Аппроксимация градиента с помощью $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ вычислительно дорого. По этой причине нет смысла выполнять проверку градиента на каждой итерации во время обучения. Это можно сделать всего несколько раз, чтобы проверить правильность расчётов градиента.\n",
    "- Проверка градиента, по крайней мере, как вы ее реализовали, не работает с dropout. Алгоритм проверки градиента обычно запускается без dropout, чтобы убедиться, что backprop правильный, а затем добавляют dropout.\n",
    "\n",
    "<font color='blue'>\n",
    "Что необходимо помнить:\n",
    "\n",
    "- Проверка градиента - это оценка близости между градиентами от обратного распространения и численной аппроксимацией градиента (вычисленной с использованием прямого распространения).\n",
    "- Проверка градиента выполняется медленно, поэтому мы не запускаем ее в каждой итерации обучения. Обычно вы запускаете его только для того, чтобы убедиться в правильности кода, а затем отключаете и используете backprop для реального процесса обучения."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit11a3618b34274847beed16472d0ddb8c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}