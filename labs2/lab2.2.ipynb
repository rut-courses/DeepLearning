{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация\n",
    "\n",
    "Модели глубокого обучения обладают большой гибкостью и набором параметров, поэтому **переобучение является серьёзной проблемой**, если обучающий набор данных недостаточно велик. Конечно, сеть хорошо работает на обучающем наборе, но **обладает плохой способностью к обобщению на новых примерах**.\n",
    "\n",
    "**В рамках данной лабораторной работы будут приобретены следующие навыки (знания):**\n",
    "- Использование регуляризации в моделях глубокого обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Данный материал опирается и использует материалы курса Deep Learning от организации deeplearning.ai`\n",
    " \n",
    " Ссылка на основной курс (для желающих получить дополнительный сертификаты): https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Пакеты/Библиотеки ##\n",
    "\n",
    "Первоначально необходимо запустить ячейку ниже, чтобы импортировать все пакеты, которые вам понадобятся во время лабораторной работы.\n",
    "- [numpy](www.numpy.org) является основным пакетом для научных вычислений в Python.\n",
    "- [matplotlib](http://matplotlib.org) это пакет для отрисовки графиков в Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) предоставляет простые и эффективные инструменты для построения моделей и анализа данных.\n",
    "- [scipy](https://www.scipy.org/) библиотека для работы со статистикой.\n",
    "- reg_utils предоставляет различные полезные функции, используемые в данной лабораторной работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec\n",
    "from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "from testCases import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Постановка задачи**: \n",
    "Представьте, что вы только что были наняты в качестве эксперта по искусственному интеллекту в футбольный клуб.\n",
    "Они хотят, чтобы вы порекомендовали позиции, где вратарь должен бить по мячу, чтобы игроки команды могли затем ударить его головой.\n",
    "\n",
    "<img src=\"images/field_kiank.png\" style=\"width:600px;height:350px;\">\n",
    "<caption><center> <u> **Рисунок 1** </u>: **Футбольное поле**<br> Вратарь бьет по мячу в воздух, игроки каждой команды борются, чтобы ударить по мячу головой </center></caption>\n",
    "\n",
    "Далее представлен следующий 2-хмерный набор данных с последних 10 игр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_2D_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая точка соответствует положению на футбольном поле, в котором футболист ударил головой по мячу после того, как голкипер выбил мяч с левой стороны футбольного поля.\n",
    "- Если точка синяя, то это означает, что игрок успел ударить по мячу головой\n",
    "- Если точка красная, это означает, что игрок другой команды ударил мяч головой\n",
    "\n",
    "**Цель**: Используйте модель глубокого обучения, чтобы найти позиции на поле, где вратарь должен бить по мячу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Анализ набора данных**: этот набор данных немного зашумлен, но похоже, что диагональная линия, отделяющая верхнюю левую половину (синюю) от нижней правой половины (красная), будет работать хорошо. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Модель без регуляризации\n",
    "\n",
    "Вы будете использовать следующую нейронную сеть (уже реализованную для вас ниже). Эту модель можно использовать:\n",
    "- в параметре *regularization mode* -- установить `lambd` в не нулевое значение. \n",
    "- в параметре *dropout mode* -- установить `keep_prob`. Величина меньше 1.\n",
    "\n",
    "Сначала необходимо попробовать модель без какой-либо регуляризации. Затем осуществить:\n",
    "- *L2 regularization* -- \"`compute_cost_with_regularization()`\" и \"`backward_propagation_with_regularization()`\"\n",
    "- *Dropout* -- \"`forward_propagation_with_dropout()`\" и \"`backward_propagation_with_dropout()`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    3-х слойнная нейронная сеть с архитектурой: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- матрица признаков (input size, number of examples)\n",
    "    Y -- вектор меток (0 - красная точка; 1 - голубая точка), с размером (output size, number of examples)\n",
    "    learning_rate -- скорость градиентного спуска \n",
    "    num_iterations -- количество итераций при обучении\n",
    "    print_cost -- True печать каждые 100 шагов\n",
    "    lambd --гиперпараметр регуляризации\n",
    "    keep_prob - вероятность сохранения нейрона активным во время drop-out\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- обученные параметры модели\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Инициализация параметров\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Прямое распространение: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Функция потерь\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Обратное распротранение\n",
    "        assert(lambd==0 or keep_prob==1)\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Обновление параметров\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Потери после итерации {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('потери')\n",
    "    plt.xlabel('итерации')\n",
    "    plt.title(\"Размер градиентного шага =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y)\n",
    "print (\"Обучающий датасет:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Тестовый датасет:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на train составляет 94,8%, а точность на тесте-91,5%. Это **baseline model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Модель без регуляризации\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель без регуляризации, очевидно, переобучена на обучающем наборе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - L2 регуляризация\n",
    "\n",
    "Стандартный способ борьбы с переобучением называется **L2 регуляризация**. Он состоит из соответствующего изменения функции затрат с:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "на:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "**Упражнение** Реализуйте `compute_cost_with_regularization()` которая вычисляет значение функции потерь по формуле (2). Вычислите $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$, используя:\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Обратите внимание, что необходимо сделать это для $W^{[1]}$, $W^{[2]}$ и $W^{[3]}$, затем сложите все три члена и умножьте на $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: compute_cost_with_regularization\n",
    "\n",
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Функция потерь с L2 регуляризацией\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- выход прямого распространения, размер (output size, number of examples)\n",
    "    Y -- вектор меток (output size, number of examples)\n",
    "    parameters -- словарь python, содержащий параметры модели\n",
    "    \n",
    "    Returns:\n",
    "    cost - значение регуляризованной функции потерь\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y)\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (примерно 1 срока кода)\n",
    "    L2_regularization_cost = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3, Y_assess, parameters = compute_cost_with_regularization_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **cost**\n",
    "    </td>\n",
    "        <td>\n",
    "    1.78648594516\n",
    "    </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, поскольку была изменена функция потерь, должно быть изменено обратное распространение. Все градиенты должны быть рассчитаны с учетом этой новой функцией потерь.\n",
    "\n",
    "**Упражнение** Реализуйте изменения, необходимые в обратном распространении, чтобы учесть регуляризацию. Изменения касаются только dW1, dW2 и dW3. Для каждого из них необходимо добавить градиент секуляризационного члена ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: backward_propagation_with_regularization\n",
    "\n",
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Функция обратного распространения базовой модели, к которой добавлена регуляризация L2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- матрица признаков(input size, number of examples)\n",
    "    Y -- вектор меток (output size, number of examples)\n",
    "    cache -- кэш из функции forward_propagation()\n",
    "    lambd -- параметр регуляризации\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- словарь с градиентами относительно каждого параметра, переменных активации\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (примерно 1 срока кода)\n",
    "    dW3 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (примерно 1 срока кода)\n",
    "    dW2 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (примерно 1 срока кода)\n",
    "    dW1 = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()\n",
    "\n",
    "grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = 0.7)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"dW3 = \"+ str(grads[\"dW3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **dW1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.25604646  0.12298827 -0.28297129]\n",
    " [-0.17706303  0.34536094 -0.4410571 ]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **dW2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.79276486  0.85133918]\n",
    " [-0.0957219  -0.01720463]\n",
    " [-0.13100772 -0.03750433]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **dW3**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-1.77691347 -0.11832879 -0.09397446]]\n",
    "    </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите модель с L2 регуляризацией $(\\lambda = 0.7)$ `model()`: \n",
    "- `compute_cost_with_regularization` вместо `compute_cost`\n",
    "- `backward_propagation_with_regularization` вместо `backward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, lambd = 0.7)\n",
    "print (\"Обучающий датасет:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Тестовый датасет:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на тестовой выборке увеличилась до 93%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Модель с L2-регуляризацией\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Наблюдения**:\n",
    "-  $\\lambda$ это гиперпараметр, который необходимо подбирать.\n",
    "- L2 регуляризация делает границы решения более гладкими. Если $\\lambda$ имеет сильно большое значение, в результате чего получается модель с большим смещением.\n",
    "\n",
    "**Что на самом деле делает L2-регуляризация?**:\n",
    "\n",
    "L2-регуляризация основана на предположении, что модель с небольшими весами проще, чем с большими. Таким образом, штрафуя квадраты значений весов в функции потерь, все веса приводятся к меньшим значениям. Это становится затратно для функции стоимости иметь слишком большие веса.\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**Что необходимо помнить:** -- влияние L2-регуляризации на:\n",
    "- Вычисление функции потерь:\n",
    "    - К стоимости добавляется член регуляризации\n",
    "- Функцию обратного распространения:\n",
    "    - Существуют дополнительные члены в градиентах относительно весовых матриц\n",
    "- Веса в конечном итоге становятся меньше (\"weight decay\"): \n",
    "    - Значение весов уменьшается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Dropout\n",
    "\n",
    "Наконец, **dropout** - это широко используемый метод регуляризации, специфичный для глубокого обучения. \n",
    "**Он случайным образом отключает некоторые нейроны на каждой итерации** Посмотрите эти два видео, чтобы понять, что это значит!\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "<br>\n",
    "<caption><center> <u> Рисунок 2 </u>: Drop-out во втором скрытом слое. <br> \n",
    "    На каждой итерации вы закрываете (=устанавливаете на ноль) каждый нейрон слоя с вероятностью $1 - keep\\_prob$ или держите его с вероятностью $keep\\_prob$ (здесь $keep\\_prob =$ 50%). Отброшенные нейроны не участвуют в обучении как в прямом, так и в обратном распространении.\n",
    " </center></caption>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center> <u> Рисунок 3 </u>: Drop-out в первом и третьем скрытом слое. <br> $1^{ый}$ слой: Отключается в среднем 40% нейронов.  $3^{ий}$ слой: Отключается в среднем 20% нейронов.. </center></caption>\n",
    "\n",
    "Когда вы отключаете некоторые нейроны, вы фактически изменяете свою модель. Идея drop-out заключается в том, что на каждой итерации вы тренируете другую модель, которая использует только подмножество нейронов. При drop-out нейроны, таким образом, становятся менее чувствительными к активации других нейронов, потому что этот другой нейрон может быть выключен в любой момент.\n",
    "\n",
    "### 3.1 - Прямое распространение с dropout\n",
    "\n",
    "**Упражнение** Реализовать прямое распространение с dropout. Добавить dropout к первому и второму скрытым слоям.\n",
    "\n",
    "**Инструкции**:\n",
    "Для поставленной задачи вам предстоит выполнить 4 шага\n",
    "1. В лекции мы решили создать переменную $d^{[1]}$ с той же формой, что и $a^{[1]}$, используя `np.random.rand()` для случайного получения чисел от 0 до 1. Здесь вы будете использовать векторизованную реализацию, поэтому создайте случайную матрицу $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ той же размерности, что и $A^{[1]}$.\n",
    "2. Установите для каждой записи $D^{[1]}$ значение 0 с вероятностью (`1-keep_prob`) или 1 с вероятностью (`keep_prob`), задав пороговые значения в $D^{[1]}$ соответствующим образом. Подсказка: чтобы установить все записи матрицы X в 0 (если запись меньше 0,5) или 1 (Если запись больше 0,5), вы должны сделать: `X = (X < 0,5)`. Обратите внимание, что 0 и 1 соответственно эквивалентны False и True.\n",
    "3. Установите $A^{[1]}$ в $A^{[1]} * D^{[1]}$. (Вы отключаете некоторые нейроны). Представить $D^{[1]}$ как маску, так что, когда она умножается на другую матрицу, она закрывает некоторые из значений.\n",
    "4. Разделите $A^{[1]}$ на `keep_prob`. Делая это, вы гарантируете, что результат затрат по-прежнему будет иметь то же ожидаемое значение, что и без drop-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: forward_propagation_with_dropout\n",
    "\n",
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Реализация прямого распространения: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- матрица признаков (2, number of examples)\n",
    "    parameters -- словарь python, содержащий параметры \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- матрица весов в 1 слое (20, 2)\n",
    "                    b1 -- вектор смещений в 1 слое (20, 1)\n",
    "                    W2 -- матрица весов во 2 слое (3, 20)\n",
    "                    b2 -- вектор смещений во 2 слое (3, 1)\n",
    "                    W3 -- матрица весов в 3 слое (1, 3)\n",
    "                    b3 -- вектор смещений в 3 слое (1, 1)\n",
    "    keep_prob - вероятность сохранения активности нейрона drop-out\n",
    "    \n",
    "    Returns:\n",
    "    A3 -- последнее значение активации, выход с прямого распространения (1,1)\n",
    "    cache -- кортеж, информация, хранящаяся для вычисления обратного распространения\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (примерно 4 строки кода) # Шаги 1-4 соответствует шагам 1-4 описанным выше. \n",
    "    D1 = None # Шаг 1: Инициализация матрицы D1 = np.random.rand(..., ...)\n",
    "    D1 = None          # Шаг 2: конвертация D1 к 0 или 1 (используя keep_prob как порог)\n",
    "    A1 = None                      # Шаг 3: отключить несколько нейронов\n",
    "    A1 = None              # Шаг 4: масштабируйте значение нейронов, которые еще не были отключены\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (примерно 4 строки кода)\n",
    "    D2 = None # Шаг 1: Инициализация матрицы D2 = np.random.rand(..., ...)\n",
    "    D2 = None   # Шаг 2: конвертация D2 к 0 или 1 (используя keep_prob как порог)\n",
    "    A2 = None              # Шаг 3: отключить несколько нейронов для A2\n",
    "    A2 = None       # Шаг 4: масштабируйте значение нейронов, которые еще не были отключены\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, parameters = forward_propagation_with_dropout_test_case()\n",
    "\n",
    "A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = 0.7)\n",
    "print (\"A3 = \" + str(A3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **A3**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.36974721  0.00305176  0.04565099  0.49683389  0.36974721]]\n",
    "    </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Обратное распространение с drop-out\n",
    "\n",
    "**Упражнение** Реализуйте обратное распространение с drop-out. Добавьте dropout к первому и второму скрытым слоям, используя маски $D^{[1]}$ и $D^{[2]}$, хранящиеся в cache.\n",
    "\n",
    "**Инструкция**:\n",
    "Обратное распространение с dropout делается в 2 шага:\n",
    "1. Ранее отключили некоторые нейроны во время прямого распространения, применив маску $D^{[1]}$ к `A1`. При обратном распространении придется отключить те же самые нейроны, повторно применив ту же самую маску $D^{[1]}$ к `dA1`. \n",
    "2. Во время прямого распространения  `A1` разделили на `keep_prob`. В методе обратного распространения ошибки необходимо делить `dA1` на `keep_prob` снова (смысл заключается в том, что если $A^{[1]}$ масштабируется на `keep_prob`, то ее производная $dA^{[1]}$ тоже масштабируется от одного и того же `keep_prob`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: backward_propagation_with_dropout\n",
    "\n",
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространение для baseline model в которой добавлен dropout.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- матрица признаков (2, number of examples)\n",
    "    Y -- вектор меток (output size, number of examples)\n",
    "    cache -- cache выход из forward_propagation_with_dropout()\n",
    "    keep_prob - вероятность сохранения активности нейрона drop-out\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- словарь с градиентами относительно каждого параметра и функции активации\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    dA2 = None             # Шаг 1: Применить маску D2 чтобы отключить те же нейроны, что и при прямом распространении\n",
    "    dA2 = None      # Шаг 2: масштабируйте значение нейронов, которые еще не были отключены\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    dA1 = None              # Шаг 1: Применить маску D1 чтобы отключить те же нейроны, что и при прямом распространении\n",
    "    dA1 = None       # Шаг 2: масштабируйте значение нейронов, которые еще не были отключены\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()\n",
    "\n",
    "gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = 0.8)\n",
    "\n",
    "print (\"dA1 = \" + str(gradients[\"dA1\"]))\n",
    "print (\"dA2 = \" + str(gradients[\"dA2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Ожидаемый результат**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **dA1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]\n",
    " [ 0.65515713  0.         -0.00337459  0.         -0.        ]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **dA2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]\n",
    " [ 0.          0.53159854 -0.          0.53159854 -0.34089673]\n",
    " [ 0.          0.         -0.00292733  0.         -0.        ]]\n",
    "    </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим модель с dropout (`keep_prob = 0.86`). Это означает, что на каждой итерации отключается каждый нейрон слоя 1 и 2 с вероятностью 24%. `model()` использует:\n",
    "- `forward_propagation_with_dropout` вместо `forward_propagation`.\n",
    "- `backward_propagation_with_dropout` вместо `backward_propagation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n",
    "print (\"Обучающий датасет:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Тестовый датасет:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на тестовой выборке увеличилась до 95%. Модель не слишком подходит для обучающего набора и отлично справляется с тестовым набором. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Модель с dropout\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Заметки**:\n",
    "- **Распространенная ошибка** при использовании dropout заключается в использовании его как для обучающей, так и для тестовой выборки. Необходимо использовать dropout (случайное исключение узлов) только для обучающей выборки.\n",
    "- Фреймворки глубокого обучения: [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) или [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) использует готовую реализацию для dropout слоев.\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**Что необходимо помнить о dropout:**\n",
    "- Dropout техника регуляризации.\n",
    "- Использовать данную технику только для обучения. Не использовать dropout для тестовой выборки\n",
    "- Применение dropout для прямого и обратного распространения\n",
    "- Во время обучения разделите каждый слой dropout на keep_prob, чтобы сохранить одно и то же ожидаемое значение для активаций. Например, если keep_prob равен 0.5, то мы в среднем отключим половину узлов, поэтому выход будет масштабирован на 0.5, поскольку только оставшаяся половина вносит свой вклад в решение. Деление на 0.5 эквивалентно умножению на 2. Следовательно, выход теперь имеет то же самое ожидаемое значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты 3-х моделей**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **model**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Точность на обучении**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Точность на тесте**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-слойнная NN без регуляризации\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "        <td>\n",
    "        91.5%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-слойнная NN с L2-регуляризации\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-слойнная NN с dropout\n",
    "        </td>\n",
    "        <td>\n",
    "        93%\n",
    "        </td>\n",
    "        <td>\n",
    "        95%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что регуляризация вредит качеству на обучающем наборе!\n",
    "Это связано с тем, что есть ограничение способность сети перестраиваться на обучающий набор.\n",
    "Но поскольку это в конечном счете дает лучшую точность на тесте, это помогает системе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Что необходимо помнить**:\n",
    "- Регуляризация помогает уменьшить эффект переобучения.\n",
    "- Регуляризация приводит веса к более низким значениям.\n",
    "- L2 регуляризация и drop-out есть два очень эффективных метода регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ссылки**:\n",
    "- Курс Deep Learning; https://www.coursera.org/specializations/deep-learning"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "SXQaI",
   "launcher_item_id": "UAwhh"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}