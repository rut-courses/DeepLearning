{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация\n",
    "\n",
    "Инициализация весов в нейронной сети позволит улучшить и ускорить процесс обучения.\n",
    "В этой лабораторной работе рассматриваются, как разные инициализации приводят к разным результатам.\n",
    "\n",
    "**В рамках данной лабораторной работы будут приобретены следующие навыки (знания):**\n",
    "- Реализация ускорения сходимости градиентного спуска\n",
    "- Инициализация весов для увеличения шансов сходимости градиентного спуска к более низкой ошибке обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Данный материал опирается и использует материалы курса Deep Learning от организации deeplearning.ai`\n",
    " \n",
    " Ссылка на основной курс (для желающих получить дополнительный сертификаты): https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Пакеты/Библиотеки\n",
    "\n",
    "Первоначально необходимо запустить ячейку ниже, чтобы импортировать все пакеты, которые вам понадобятся во время лабораторной работы.\n",
    "- [numpy](www.numpy.org) является основным пакетом для научных вычислений в Python.\n",
    "- [matplotlib](http://matplotlib.org) это пакет для отрисовки графиков в Python.\n",
    "- [sklearn](http://scikit-learn.org/stable/) предоставляет простые и эффективные инструменты для построения моделей и анализа данных.\n",
    "- init_utils предоставляет различные полезные функции, используемые в данной лабораторной работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
    "from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 6.0) # размер графика\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Загрузка и отображение данных\n",
    "train_X, train_Y, test_X, test_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**: Построить классификатор, который разделяет красные и синие точки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Модель нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лабораторной работе используется 3-слойнная нейронная сеть (уже реализована).\n",
    "Вот методы инициализации, с которыми необходимо будет экспериментировать:\n",
    "- *Инициализация в ноль* -- установить `initialization = \"zeros\"` как входной аргумент функции.\n",
    "- *Случайная инициализация* -- установить `initialization = \"random\"` как входной аргумент функции. Это инициализирует веса к большим случайным значениям.\n",
    "- *He инициализация* -- установить `initialization = \"he\"` как входной аргумент функции. Это инициализирует веса к случайным значениям по правилу предложенному авторами статьи He et al., 2015. \n",
    "\n",
    "**Инструкции**: Пожалуйста, прочитайте приведенный ниже код и запустите его. В следующей части необходимо будет реализовать три метода инициализации для данной модели `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n",
    "    \"\"\"\n",
    "    3-х слойнная нейронная сеть с архитектурой: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- матрица признаков (2, number of examples)\n",
    "    Y -- вектор меток (0 - красная точка; 1 - голубая точка), с размером (1, number of examples)\n",
    "    learning_rate -- скорость градиентного спуска \n",
    "    num_iterations -- количество итераций при обучении\n",
    "    print_cost -- True печать каждые 100 шагов\n",
    "    initialization -- тип инициализации весов (\"zeros\",\"random\" или \"he\")\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- обученные параметры модели\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = [] # отслеживание потерь\n",
    "    m = X.shape[1] # количество примеров\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "    \n",
    "    # Инициализация параметров\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Градиентный спуск\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Прямое распространение: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Потери\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Обратное распространение\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Обновление параметров\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Печать текущей итерации\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Потери после итерации {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Размер градиентного шага =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Инициализация в ноль\n",
    "\n",
    "Существует два типа параметров, которые инициализируются в нейронной сети:\n",
    "- матрица весов $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- векора смещений $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "\n",
    "**Упражнение** Реализовать следующую функцию для инициализации параметров в ноль.\n",
    "Позже можно будет увидеть, что это работает плохо, так как процесс обучения весов не может \"нарушить симметрию\".\n",
    "Используйте np.zeros((..,..)) с корректными размерами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_parameters_zeros \n",
    "\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python массив (list), который содержит информацию о размере каждого слоя.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь с параметрами \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- матрица весов с размером 1-го слоя (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- вектор смещений 1-го слоя (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- матрица весов с размером L-го слоя (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- вектор смещений L-го слоя (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims) # количество слоёв в нейронной сети\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.  0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите следующий код для обучения модели на 15 000 итерациях с использованием инициализации в ноль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"Обучающий датасет:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Тестовый датасет:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество плохое, и размер потерь не уменьшается, и алгоритм работает не лучше, чем случайное угадывание. \n",
    "\n",
    "Почему? \n",
    "\n",
    "Необходимо рассмотреть детали совершаемых предсказаний и границы принятия решений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"predictions_train = \" + str(predictions_train))\n",
    "print (\"predictions_test = \" + str(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Модель с инициализацией в ноль\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель предсказывает 0 для каждого примера. \n",
    "\n",
    "В общем случае инициализация всех весов до нуля приводит к тому, что сеть не может нарушить симметрию.\n",
    "Это означает, что каждый нейрон в каждом слое будет изучать одно и то же, и вы можете с таким же успехом обучать нейронную сеть с $n^{[l]}=1$ для каждого слоя, и эта сеть не более мощна, чем линейный классификатор, такой как логистическая регрессия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**Что необходимо помнить:**\n",
    "    \n",
    "- Матрица весов $W^{[l]}$ должна быть инициализирована случайным образом для нарушения симметрии.\n",
    "- Однако можно инициализировать смещения $b^{[l]}$ в ноль. Симметрия все ещё будет нарушена, пока $W^{[l]}$ инициализируется случайным образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Случайная инициализация\n",
    "\n",
    "Чтобы нарушить симметрию, давайте произвольно инициализируем веса. В этом упражнении продемонстрирован процесс того, что происходит, если веса инициализируются случайным образом, но с очень большими значениями.\n",
    "\n",
    "**Упражнение** Реализуйте следующую функцию для инициализации весов до больших случайных значений (масштабированных на \\*10) и смещений в ноль. Используйте `np.random.randn(..,..) * 10` для весов и `np.zeros((.., ..))` для смещений. Мы фиксируем случайную инициализацию `np.random.seed(..)`  чтобы убедиться, что ваши \"случайные\" веса совпадают с ответами, поэтому не переживайте, если при запуске несколько раз код дает всегда одни и те же начальные значения параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_parameters_random\n",
    "\n",
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python массив (list), который содержит информацию о размере каждого слоя.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь с параметрами \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- матрица весов с размером 1-го слоя (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- вектор смещений 1-го слоя (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- матрица весов с размером L-го слоя (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- вектор смещений L-го слоя (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    for l in range(1, L):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 17.88628473   4.36509851   0.96497468]\n",
    " [-18.63492703  -2.77388203  -3.54758979]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.82741481 -6.27000677]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите следующий код для обучения модели на 15 000 итерациях с использованием случайной инициализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"random\")\n",
    "print (\"Обучающий датасет:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Тестовый датасет:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы видите \"inf\" как значение функции потерь после итерации 0, это происходит из-за численного округления; более сложная численная реализация исправит это. \n",
    "\n",
    "В любом случае можно наблюдать нарушение симметрии, и это дает лучшие результаты, чем раньше. Модель больше не выводит всё в 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions_train)\n",
    "print (predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Модель со случайной инициализацией\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения**:\n",
    "- Значение функции потерь начинается с большого значения. Причина этого в том, что при больших случайных весах функция активации (sigmoid) выводит результаты, которые очень близки к 0 или 1 для некоторых примеров, и когда он получает этот пример неправильно, значение функции потерь имеет большое значение. Действительно, когда $\\log(a^{[3]}) = \\log(0)$, потеря идет в бесконечность.\n",
    "- Плохая инициализация может привести к исчезновению/взрыву градиентов, что также замедляет алгоритм оптимизации.\n",
    "- Если данную сеть обучать дольше, то вы увидите лучшие результаты, но инициализация с чрезмерно большими случайными числами замедляет оптимизацию.\n",
    "\n",
    "<font color='blue'>\n",
    "    \n",
    "**Что необходимо помнить:**\n",
    "- Инициализация весов для очень больших случайных значений работает не достаточно хорошо.\n",
    "- Возможно, что инициализация с небольшими случайными значениями будет лучше. Важный вопрос: насколько малыми должны быть эти случайные величины? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - He инициализация\n",
    "\n",
    "В данной части необходимо реализовать \"He инициализацию\"; названа в честь первого автора статьи He et al., 2015. (Если вам известно \"Xavier инициализация\", \"He инициализация\" похожа за исключением того, что инициализация Xavier использует следующий масштабирующий коэффициент для весов $W^{[l]}$ l слоя `sqrt(1./layers_dims[l-1])`, а He инициализация использует `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "**Упражнение** Реализовать следующую функцию для \"He инициализации\".\n",
    "\n",
    "**Подсказка**: Данная функция похожа на предыдущую `initialize_parameters_random(...)`. Разница лишь в том, что вместо умножения `np.random.randn(..,..)` на 10, необходимо умножить на $\\sqrt{\\frac{2}{\\text{размер предыдущего слоя}}}$, именно это He рекомендует для слоев с функцией активацией ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_parameters_he\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python массив (list), который содержит информацию о размере каждого слоя.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь с параметрами \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- матрица весов с размером 1-го слоя (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- вектор смещений 1-го слоя (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- матрица весов с размером L-го слоя (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- вектор смещений L-го слоя (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 1.78862847  0.43650985]\n",
    " [ 0.09649747 -1.8634927 ]\n",
    " [-0.2773882  -0.35475898]\n",
    " [-0.08274148 -0.62700068]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите следующий код для обучения модели на 15 000 итерациях с использованием He инициализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"he\")\n",
    "print (\"Обучающий датасет:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Тестовый датасет:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Модель He инициализацией\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения**:\n",
    "- Модель с He инициализацией разделяет красные и синие точки достаточно хорошо за небольшое количество итераций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 - Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Было исследовано три различных типа инициализации. Для одного и того же числа итераций и одних и тех же гиперпараметров:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Модель**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Точность на обучающей выборке**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Проблемма/Комментарий**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-слойнная NN с инициализацией в ноль\n",
    "        </td>\n",
    "        <td>\n",
    "        50%\n",
    "        </td>\n",
    "        <td>\n",
    "        проблема в разрушении симметрии\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-слойнная NN со случайной инициализацией\n",
    "        </td>\n",
    "        <td>\n",
    "        83%\n",
    "        </td>\n",
    "        <td>\n",
    "        большие веса при инициализации \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        33-слойнная NN с He инициализацией\n",
    "        </td>\n",
    "        <td>\n",
    "        99%\n",
    "        </td>\n",
    "        <td>\n",
    "        рекомендуемый метод\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**Что необходимо помнить**:\n",
    "- Разные инициализации приводят к разным результатам\n",
    "- Случайная инициализация используется, чтобы нарушить симметрию и убедиться, что различные скрытые нейронные могут обучиться разным вещам\n",
    "- He инициализация хорошо работает для сетей с функцией активации ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ссылки**:\n",
    "- Курс Deep Learning; https://www.coursera.org/specializations/deep-learning"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "XOESP",
   "launcher_item_id": "8IhFN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}