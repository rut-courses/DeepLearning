{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы оптимизации\n",
    "\n",
    "В качестве метода обновления параметров модели и минимизации функции потерь в курсе используется метод градиентного спуска. В данной лабораторной работе рассматриваются наиболее усовершенствованные методы оптимизации, которые позволяют увеличить скорость обучения и достичь наулучшего значения функции потерь. Хороший метод оптимизации позволит ускорит обучение с нескольких дней до нескольких часов.\n",
    "\n",
    "Градиентный спуск осуществляет поиск минимума функции потерь $J$. Можно представить себе это следующим образом:\n",
    "<img src=\"images/cost.jpg\" style=\"width:650px;height:300px;\">\n",
    "<caption><center> <u> **Рисунок 1** </u>: **Минимизация потерь как поиск самой низкой точки в долине**<br> На каждом этапе обучения необходимо обновлять параметры, следуя определенному направлению, чтобы попытаться достичь минимально возможной точки. </center></caption>\n",
    "\n",
    "**Определение**: $\\frac{\\partial J}{\\partial a } = $ `da` для любой переменной `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from testCases import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # установить дефолтный размер графиков\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Градиентный спуск\n",
    "\n",
    "Самый простой метод оптимизации в машинном обучении это градиентный спуск (gradient descent - GD), когда делаются градиентные шаги по отношению ко всем $m$-примерам на каждой итерации - также можно встретить название  Пакетный градиентный спуск (Batch Gradient Descent). \n",
    "\n",
    "**Упражнение**: Реализовать правило обновления градиентного спуска. Градиентный спуск для $l = 1, ..., L$: \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}$$\n",
    "\n",
    "где L - это количество слоев и $\\alpha$ скорость градиентного спуска. Все параметры должны быть сохранены в словарь `parameters`. \n",
    "Обратите внимание, что при итерировании по `l` цикл `for` начинается с 0, в то время первые параметры начинаются с $W^{[1]}$ и $b^{[1]}$. Вам необходимо сдвинуть `l` в `l+1` при написании функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: update_parameters_with_gd\n",
    "\n",
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Обновление параметров с использованием одного шага GD\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры, которые должны быть обновлены:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python словарь, содержащий градиенты для обновления каждого параметра:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- скорость градиентного спуска.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь, содержащий обновлённые параметры \n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # количество слоёв в нейронной сети\n",
    "    # Правило обновления каждого параметра\n",
    "    for l in range(L):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td > **W1** </td> \n",
    "          <td > [[ 1.63535156 -0.62320365 -0.53718766]\n",
    " [-1.07799357  0.85639907 -2.29470142]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **b1** </td> \n",
    "           <td > [[ 1.74604067]\n",
    " [-0.75184921]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **W2** </td> \n",
    "           <td > [[ 0.32171798 -0.25467393  1.46902454]\n",
    " [-2.05617317 -0.31554548 -0.3756023 ]\n",
    " [ 1.1404819  -1.09976462 -0.1612551 ]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **b2** </td> \n",
    "           <td > [[-0.88020257]\n",
    " [ 0.02561572]\n",
    " [ 0.57539477]] </td> \n",
    "    </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо пакетного градиентного спуска, часто в задачах используется Стохастический градиентный спуск (SGD), который эквивалентен  мини-пакетному (mini-batch) градиентному спуску, использующий на каждой итерации в качестве мини-пакета только 1 пример. Реализованное ранее правило обновления параметров не изменяется. Что изменится, так это то, что необходимо вычислять градиенты только на одном примере из обучающей выборки за один раз, а не на всем учебном наборе. Приведенные ниже примеры кода иллюстрируют разницу между стохастическим градиентным спуском и (пакетным) градиентным спуском.\n",
    "\n",
    "- **(пакетный) Градиентный спуск**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Прямое распространение\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Вычисление потерь\n",
    "    cost = compute_cost(a, Y)\n",
    "    # Обратное распростарнение\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Обновление параметров\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "- **Стохастический градиентный спуск**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m):\n",
    "        # Прямое распространение\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Вычисление потерь\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        # Обратное распростарнение\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Обновление параметров\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Стохастическом градиентном спуске используется 1 обучающий пример для обновления параметров. Использование SGD обусловлено скоростью вычислений при большом обучающем наборе. Но при этом во время обучения параметры будут \"колебаться\", а не сходиться плавно. Вот иллюстрация этого:\n",
    "\n",
    "<img src=\"images/kiank_sgd.png\" style=\"width:750px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Рисунок 1** </u><font color='purple'>  : **SGD против GD**<br> \"+\" обозначает точку минимума. SGD приводит ко многим колебаниям, чтобы достичь сходимости. Но каждый шаг намного быстрее вычисляется для SGD, чем для GD, так как он использует только один обучающий пример. </center></caption>\n",
    "\n",
    "**Обратите внимание** реализация SGD включает 3 три цикла:\n",
    "1. Количество итераций обучения\n",
    "2. Количество обучающих примеров - $m$\n",
    "3. Количество слоев (обновление всех параметров, с $(W^{[1]},b^{[1]})$ до $(W^{[L]},b^{[L]})$)\n",
    "\n",
    "На практике для получения результатов наиболее быстрым способом, обычно используют мини-пакетный градиентный спуск, который включает в себя количество примеров $mini_batch_size$, где значения $mini_batch_size$ лежат в интервале $1 < mini_batch_size < m$ для каждого шага.\n",
    "\n",
    "<img src=\"images/kiank_minibatch.png\" style=\"width:750px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Рисунок 2** </u>: <font color='purple'>  **SGD против Мини-пакетный GD**<br> \"+\" обозначает точку минимума. Использование мини-пакетов в алгоритме часто приводит к более быстрой оптимизации.</center></caption>\n",
    "\n",
    "<font color='blue'>\n",
    "**Что необходимо помнить**:\n",
    "- Разница между градиентным спуском, мини-пакетным градиентным спуском и стохастическим градиентным спуском заключается в количестве примеров, используемых для выполнения одного шага обновления параметров.\n",
    "- Необходимо настраивать гиперпараметр - скорость обучения $\\alpha$.\n",
    "- При хорошо подобранном размере мини-пакета результат для обучения превосходит классический градиентный спуск, либо стохастический градиентный спуск (особенно когда обучающий набор включает множество обучающих примеров)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Mини-пакетный градиентный спуск\n",
    "\n",
    "В данной части Вы будете создавать мини-пакеты из обучающего набора (X, Y).\n",
    "\n",
    "Два шага:\n",
    "- **Перемешивание**: создание перемешанной выборки из обучающего набора (X, Y). Каждая колонка X и Y представляет один обучающий пример. Обратите внимание, что случайное перемешивание выполняется синхронно между X и Y. \n",
    "\n",
    "Таким образом, после перетасовки  $i^{th}$ столбец матрицы X является одним примером, соответствующим $i^{th}$ метке в Y. Шаг перетасовки гарантирует, что примеры будут случайным образом разбиты на различные мини-пакеты.\n",
    "\n",
    "<img src=\"images/kiank_shuffle.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "- **Разделение**: разделение перемешанных (X, Y) на мини-пакеты размером mini_batch_size (в примере используется 64). Обратите внимание, что количество обучающих примеров не всегда делится на mini_batch_size. Последняя мини-партия может быть меньше. Если конечный мини-пакет меньше, чем полный mini_batch_size, он будет выглядеть следующим образом:\n",
    "\n",
    "<img src=\"images/kiank_partition.png\" style=\"width:550px;height:300px;\">\n",
    "\n",
    "**Упражнение**: Реализуйте `random_mini_batches`. Часть кода с шагом перетасовки уже реализовано. Ваша задача дописать шаг разделения,ниже написан код, который выбирает индексы для мини-пакетов $1^{й}$ и $2^{й}$: \n",
    "```python\n",
    "first_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\n",
    "second_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n",
    "...\n",
    "```\n",
    "Обратите внимание, что последний мини-пакет может оказаться меньше, чем `mini_batch_size=64`. Пусть $\\lfloor s \\rfloor$ обозначает $s$ округленное до ближайшего целого числа (`math.floor(s)` в Python). Если общее число примеров не кратно `mini_batch_size = 64` то будет $\\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$ мини-пакеты с полными 64 примерами, а число примеров в конечном мини-пакете будет  ($m-mini_\\_batch_\\_size \\times \\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Создание списка со случайными мини-пакетами из (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- входные данные, размер (input size, number of examples)\n",
    "    Y -- вектор меток (1 for blue dot / 0 for red dot), размер (1, number of examples)\n",
    "    mini_batch_size -- размер мини-пакета, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- список мини-пакетов (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]                  # количество обучающих примеров\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Перемешиваем (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Разделяем (shuffled_X, shuffled_Y). За исключением конечного случая.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # количество мини-пакетов с размером mini_batch_size в разделении\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        mini_batch_X = None\n",
    "        mini_batch_Y = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Обработка конечного случая (mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        mini_batch_X = None\n",
    "        mini_batch_Y = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "\n",
    "print (\"размер 1го mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"размер 2го mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"размер 3го mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"размер 1го mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"размер 2го mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"размер 3го mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"Мини пакетная проверка: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:50%\"> \n",
    "    <tr>\n",
    "    <td > **shape of the 1st mini_batch_X** </td> \n",
    "           <td > (12288, 64) </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **shape of the 2nd mini_batch_X** </td> \n",
    "           <td > (12288, 64) </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **shape of the 3rd mini_batch_X** </td> \n",
    "           <td > (12288, 20) </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td > **shape of the 1st mini_batch_Y** </td> \n",
    "           <td > (1, 64) </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **shape of the 2nd mini_batch_Y** </td> \n",
    "           <td > (1, 64) </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **shape of the 3rd mini_batch_Y** </td> \n",
    "           <td > (1, 20) </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **Мини пакетная проверка** </td> \n",
    "           <td > [ 0.90085595 -0.7612069   0.2344157 ] </td> \n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Что необходимо помнить**:\n",
    "- Перемешивание и разделение - это два шага, необходимые для создания и использования мини-пакетов.\n",
    "- Степени двух часто выбираются в качестве размера мини-пакетов, например, 16, 32, 64, 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Momentum\n",
    "\n",
    "Поскольку мини-пакетный градиентный спуск делает обновление параметров после просмотра только подмножества примеров, направление обновления имеет некоторую дисперсию, и поэтому путь, пройденный мини-пакетным градиентным спуском, будет \"колебаться\" в сторону точки минимума. Использование импульса (momentum) может уменьшить эти колебания.\n",
    "\n",
    "Импульс учитывает прошлые градиенты, чтобы сгладить обновление. Мы сохраним \"направление\" предыдущих градиентов в переменной $v$. Формально это будет экспоненциально взвешенное среднее градиента на предыдущих шагах. Вы также можете думать о $v$ как о \"скорости\" шара, катящегося вниз по склону, наращивая скорость (и импульс) в соответствии с направлением градиента/наклона холма.\n",
    "\n",
    "<img src=\"images/opt_momentum.png\" style=\"width:400px;height:250px;\">\n",
    "<caption><center> <u><font color='purple'>**Рисунок 3**</u><font color='purple'>: \n",
    "Красные стрелки показывают направление, выбранное одним шагом мини-пакетного градиентного спуска с импульсом. Синие точки показывают направление градиента (относительно текущего мини-пакета) на каждом шаге. Вместо того чтобы просто следовать градиенту, мы позволяем градиенту влиять на $v$, а затем делаем шаг в направлении $v$.<br> <font color='black'> </center>\n",
    "\n",
    "\n",
    "**Упражнение**: Инициализируйте скорость. Скрость $v$ -  python словарь который должен быть инциализирован в массив нулей. Его ключи такие же, как и в словаре \"grads\":\n",
    "for $l =1,...,L$:\n",
    "```python\n",
    "v[\"dW\" + str(l+1)] = ... #(numpy array of zeros с размером как parameters[\"W\" + str(l+1)])\n",
    "v[\"db\" + str(l+1)] = ... #(numpy array of zeros с размером как parameters[\"b\" + str(l+1)])\n",
    "```\n",
    "**Заметка** итератор по l начинается с 0 в цикле, в то время первый параметры начинаются с 1 - v[\"dW1\"] и v[\"db1\"]. Поэтому происходит сдвиг l в l+1 в цикле `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_velocity\n",
    "\n",
    "def initialize_velocity(parameters):\n",
    "    \"\"\"\n",
    "    Иницилизация скорости как python словаря с:\n",
    "                - ключами: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - значениями: numpy массив нулей размером соответствующим размеру градиентов/параметров.\n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python словарь, содержащий скорость\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # количество слоёв в нейронной сети\n",
    "    v = {}\n",
    "    \n",
    "    # Инициализация скорости\n",
    "    for l in range(L):\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        v[\"dW\" + str(l+1)] = None\n",
    "        v[\"db\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_velocity_test_case()\n",
    "\n",
    "v = initialize_velocity(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "    <tr>\n",
    "    <td > **v[\"dW1\"]** </td> \n",
    "           <td > [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"db1\"]** </td> \n",
    "           <td > [[ 0.]\n",
    " [ 0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"dW2\"]** </td> \n",
    "           <td > [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]\n",
    " [ 0.  0.  0.]] </td> \n",
    "    </tr> \n",
    "    \n",
    "    <tr>\n",
    "    <td > **v[\"db2\"]** </td> \n",
    "           <td > [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "    </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**:  Реализайте метод обновления параметров с использованием импульса (momentum). Правило momentum для $l = 1, ..., L$: \n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{4}$$\n",
    "\n",
    "где L количество слоев, $\\beta$ импульс и $\\alpha$ скорость обучения или длина градиентного шага. Все параметры должны быть сохранены в словарь `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: update_parameters_with_momentum\n",
    "\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    \"\"\"\n",
    "    Обновление параметров с использованием Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python словарь, содержащий градиенты по параметрам:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python словарь, содержащий скорости:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- гиперпараметр импульса\n",
    "    learning_rate -- скорость обучения\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python словарь, содержащий обновленные параметры\n",
    "    v -- словарь, содержащий обновленные скорости\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # количество слоёв в нейронной сети\n",
    "    \n",
    "    # Momentum обновление каждого параметра\n",
    "    for l in range(L):\n",
    "        \n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~4 строки кода)\n",
    "        # вычисление скрости\n",
    "        v[\"dW\" + str(l+1)] = None\n",
    "        v[\"db\" + str(l+1)] = None\n",
    "        # обновление параметров\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
    "\n",
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:90%\"> \n",
    "    <tr>\n",
    "    <td > **W1** </td> \n",
    "           <td > [[ 1.62544598 -0.61290114 -0.52907334]\n",
    " [-1.07347112  0.86450677 -2.30085497]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **b1** </td> \n",
    "           <td > [[ 1.74493465]\n",
    " [-0.76027113]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **W2** </td> \n",
    "           <td > [[ 0.31930698 -0.24990073  1.4627996 ]\n",
    " [-2.05974396 -0.32173003 -0.38320915]\n",
    " [ 1.13444069 -1.0998786  -0.1713109 ]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **b2** </td> \n",
    "           <td > [[-0.87809283]\n",
    " [ 0.04055394]\n",
    " [ 0.58207317]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"dW1\"]** </td> \n",
    "           <td > [[-0.11006192  0.11447237  0.09015907]\n",
    " [ 0.05024943  0.09008559 -0.06837279]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"db1\"]** </td> \n",
    "           <td > [[-0.01228902]\n",
    " [-0.09357694]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"dW2\"]** </td> \n",
    "           <td > [[-0.02678881  0.05303555 -0.06916608]\n",
    " [-0.03967535 -0.06871727 -0.08452056]\n",
    " [-0.06712461 -0.00126646 -0.11173103]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"db2\"]** </td> \n",
    "           <td > [[ 0.02344157]\n",
    " [ 0.16598022]\n",
    " [ 0.07420442]]</td> \n",
    "    </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Обратите внимание**:\n",
    "- Скорость инициализируется нулями. Таким образом, алгоритм займет несколько итераций, чтобы \"нарастить\" скорость и начать делать большие шаги.\n",
    "- Если $\\beta = 0$, то это просто становится стандартным градиентным спуском без импульса (momentum). \n",
    "\n",
    "**Как выбрать $\\beta$?**\n",
    "\n",
    "- Чем больше импульс $\\beta$, тем плавнее идёт процесс обновления параметров, потому что учитываются прошлые градиенты наиболее глубоко. Но если $\\beta$ слишком велик, то это может сгладить процесс обновления параметров слишком много. \n",
    "- Наиболее часто выбирают значения $\\beta$ в диапазоне от 0.8 до 0.999. \n",
    "- Настройка оптимального $\\beta$ для вашей модели требует подбора нескольких значений, чтобы отследить с каким значением модель работает лучше всего с точки зрения уменьшения значения функции затрат $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Что необходимо помнить**:\n",
    "- Импульс учитывает прошлые градиенты, чтобы сгладить шаги градиентного спуска. Он может применяться с пакетным градиентным спуском, мини-пакетным градиентным спуском или стохастическим градиентным спуском.\n",
    "- В алгоритме присутствуют гиперпараметры, которые требуют настроек: импульс $\\beta$ и скорость обучения $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Adam\n",
    "\n",
    "Adam - один из наиболее эффективных алгоритмов оптимизации для обучения нейронных сетей. Он сочетает в себе идеи из RMSProp (описанные в лекции) и Momentum.\n",
    "\n",
    "**Как работает Adam?**\n",
    "1. Он вычисляет экспоненциально взвешенное среднее прошлых градиентов и сохраняет его в переменных $v$ (до коррекции смещения (bias)) и $v^{corrected}$ (с коррекцией смещения (bias)). \n",
    "2. Он вычисляет экспоненциально взвешенное среднее квадратов прошлых градиентов и сохраняет его в переменных $s$ (до коррекции смещения (bias)) и $s^{corrected}$ (с коррекцией смещения (bias)). \n",
    "3. Он обновляет параметры в направлении, основанном на объединении информации из \"1\" и \"2\".\n",
    "\n",
    "Правило обновления для $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "где:\n",
    "- t подсчитывает количество шагов, сделанных алгоритмом\n",
    "- L количество слоёв\n",
    "- $\\beta_1$ и $\\beta_2$ являются гиперпараметрами, управляющими двумя экспоненциально взвешенными средними\n",
    "- $\\alpha$ скорость градиентного спуска\n",
    "- $\\varepsilon$ - это очень маленькое число, для исключения случая деления на ноль"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**: Инициализируйте переменные алгоритма Adam $v, s$ которые отслеживают прошлую информацию.\n",
    "\n",
    "**Инструкция**: Переменные $v, s$ - это словари python, которые необходимо инициализировать массивами нулей. Ключами является тоже самое что и в словаре `grads`:\n",
    "for $l = 1, ..., L$:\n",
    "```python\n",
    "v[\"dW\" + str(l+1)] = ... #(numpy array of zeros таким же размером как parameters[\"W\" + str(l+1)])\n",
    "v[\"db\" + str(l+1)] = ... #(numpy array of таким же размером как parameters[\"b\" + str(l+1)])\n",
    "s[\"dW\" + str(l+1)] = ... #(numpy array of таким же размером как as parameters[\"W\" + str(l+1)])\n",
    "s[\"db\" + str(l+1)] = ... #(numpy array of таким же размером как as parameters[\"b\" + str(l+1)])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: initialize_adam\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Инициализация v и s как для python словаря:\n",
    "                - ключи: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - значения: numpy arrays of zeros размером соответствующий градиентам/параметрам.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры:\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- словарь python, который будет содержать экспоненциально взвешенное среднее градиента\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- словарь python, который будет содержать экспоненциально взвешенное среднее квадратичного градиента.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "    ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~ 4 строки кода)\n",
    "        v[\"dW\" + str(l+1)] = None\n",
    "        v[\"db\" + str(l+1)] = None\n",
    "        s[\"dW\" + str(l+1)] = None\n",
    "        s[\"db\" + str(l+1)] = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_adam_test_case()\n",
    "\n",
    "v, s = initialize_adam(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "    <tr>\n",
    "    <td > **v[\"dW1\"]** </td> \n",
    "           <td > [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"db1\"]** </td> \n",
    "           <td > [[ 0.]\n",
    " [ 0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"dW2\"]** </td> \n",
    "           <td > [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]\n",
    " [ 0.  0.  0.]] </td> \n",
    "    </tr>  \n",
    "    <tr>\n",
    "    <td > **v[\"db2\"]** </td> \n",
    "           <td > [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **s[\"dW1\"]** </td> \n",
    "           <td > [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **s[\"db1\"]** </td> \n",
    "           <td > [[ 0.]\n",
    " [ 0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **s[\"dW2\"]** </td> \n",
    "           <td > [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]\n",
    " [ 0.  0.  0.]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **s[\"db2\"]** </td> \n",
    "           <td > [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]] </td> \n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение**:  Теперь необходимо реализовать обновление параметров с помощью Adam. Общее правило обновления параметров, для $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{W^{[l]}} = \\beta_1 v_{W^{[l]}} + (1 - \\beta_1) \\frac{\\partial J }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{W^{[l]}} = \\frac{v_{W^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{W^{[l]}} = \\beta_2 s_{W^{[l]}} + (1 - \\beta_2) (\\frac{\\partial J }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{W^{[l]}} = \\frac{s_{W^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{W^{[l]}}}{\\sqrt{s^{corrected}_{W^{[l]}}}+\\varepsilon}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: update_parameters_with_adam\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \"\"\"\n",
    "    Обновление параметров с использование алгоритма Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python словарь, содержащий параметры:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python словарь, содержащий градиентны параметров:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- переменная Adam, скользящее среднее первого градиента, словарь python\n",
    "    s -- переменная Adam, скользящее среднее квадратичного градиента, словарь python\n",
    "    learning_rate -- скорость обучения\n",
    "    beta1 -- экспоненциальный гиперпараметр распада для оценок первого момента\n",
    "    beta2 -- экспоненциальный гиперпараметр распада для оценок второго момента\n",
    "    epsilon -- гиперпараметр, предотвращающий деление на ноль в обновлениях Adam\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python словарь, содержащий параметры: \n",
    "    v -- переменная Adam, скользящее среднее первого градиента, словарь python\n",
    "    s -- переменная Adam, скользящее среднее квадратичного градиента, словарь python\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2            \n",
    "    v_corrected = {}                    \n",
    "    s_corrected = {}            \n",
    "    \n",
    "    for l in range(L):\n",
    "        # Скользящее среднее градиентов. Входы: \"v, grads, beta1\" Выход: \"v\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        v[\"dW\" + str(l+1)] = None\n",
    "        v[\"db\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "        # Вычислить смещение-скорректированная оценка первого момента. Входы: \"v, beta1, t\". Выход: \"v_corrected\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        v_corrected[\"dW\" + str(l+1)] = None\n",
    "        v_corrected[\"db\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "        # Скользящее среднее квадратов градиентов. Входы: \"s, grads, beta2\". Выход: \"s\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        s[\"dW\" + str(l+1)] = None\n",
    "        s[\"db\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "        # Вычислить смещение-скорректированная оценка второго момента. Входы: \"s, beta2, t\". Выход: \"s_corrected\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        s_corrected[\"dW\" + str(l+1)] = None\n",
    "        s_corrected[\"db\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "        # Обновление параметров. Входы: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Выход: \"parameters\".\n",
    "        ### НАЧАЛО ВАШЕГО КОД ЗДЕСЬ ### (~2 строки кода)\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "        ### ОКОНЧАНИЕ ВАШЕГО КОД ЗДЕСЬ ###\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
    "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый результат**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td > **W1** </td> \n",
    "           <td > [[ 1.63178673 -0.61919778 -0.53561312]\n",
    " [-1.08040999  0.85796626 -2.29409733]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **b1** </td> \n",
    "           <td > [[ 1.75225313]\n",
    " [-0.75376553]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **W2** </td> \n",
    "           <td > [[ 0.32648046 -0.25681174  1.46954931]\n",
    " [-2.05269934 -0.31497584 -0.37661299]\n",
    " [ 1.14121081 -1.09245036 -0.16498684]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **b2** </td> \n",
    "           <td > [[-0.88529978]\n",
    " [ 0.03477238]\n",
    " [ 0.57537385]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"dW1\"]** </td> \n",
    "           <td > [[-0.11006192  0.11447237  0.09015907]\n",
    " [ 0.05024943  0.09008559 -0.06837279]] </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td > **v[\"db1\"]** </td> \n",
    "           <td > [[-0.01228902]\n",
    " [-0.09357694]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"dW2\"]** </td> \n",
    "           <td > [[-0.02678881  0.05303555 -0.06916608]\n",
    " [-0.03967535 -0.06871727 -0.08452056]\n",
    " [-0.06712461 -0.00126646 -0.11173103]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **v[\"db2\"]** </td> \n",
    "           <td > [[ 0.02344157]\n",
    " [ 0.16598022]\n",
    " [ 0.07420442]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **s[\"dW1\"]** </td> \n",
    "           <td > [[ 0.00121136  0.00131039  0.00081287]\n",
    " [ 0.0002525   0.00081154  0.00046748]] </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td > **s[\"db1\"]** </td> \n",
    "           <td > [[  1.51020075e-05]\n",
    " [  8.75664434e-04]] </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td > **s[\"dW2\"]** </td> \n",
    "           <td > [[  7.17640232e-05   2.81276921e-04   4.78394595e-04]\n",
    " [  1.57413361e-04   4.72206320e-04   7.14372576e-04]\n",
    " [  4.50571368e-04   1.60392066e-07   1.24838242e-03]] </td> \n",
    "    </tr> \n",
    "    <tr>\n",
    "    <td > **s[\"db2\"]** </td> \n",
    "           <td > [[  5.49507194e-05]\n",
    " [  2.75494327e-03]\n",
    " [  5.50629536e-04]] </td> \n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у вас есть три работающих алгоритма оптимизации (мини-пакетный градиентный спуск, Momentum, Adam). Давайте реализуем модель с каждым из этих оптимизаторов и рассмотрим разницу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Модель с различными методами оптимизации\n",
    "\n",
    "Lets use the following \"moons\" dataset to test the different optimization methods. (The dataset is named \"moons\" because the data from each of the two classes looks a bit like a crescent-shaped moon.) \n",
    "\n",
    "Используется следующий набор данных \"moons\" для тестирования различных методов оптимизации. (Набор данных называется \"moons\", потому что данные из каждого класса немного похожи на полумесяц луны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже реализовали 3-х слойную нейронную сеть. Вы будете тренировать его с помощью:\n",
    "- Мини-пакетным **Градиентным спуском**:\n",
    "    - `update_parameters_with_gd()`\n",
    "- Мини-пакетным **Momentum**:\n",
    "    - `initialize_velocity()` и `update_parameters_with_momentum()`\n",
    "- Мини-пакетным **Adam**:\n",
    "    - `initialize_adam()` и `update_parameters_with_adam()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-х слойная нейросетевая модель, которая может быть запущена в различных режимах оптимизатора.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- матрица признаков (2, number of examples)\n",
    "    Y -- вектор меток (1 for blue dot / 0 for red dot), размер (1, number of examples)\n",
    "    layers_dims -- python список, содержащий размер каждого слоя\n",
    "    learning_rate -- скорость градиентного спуска.\n",
    "    mini_batch_size -- размер мини-пакета\n",
    "    beta -- Momentum гиперпараметр\n",
    "    beta1 -- экспоненциальный гиперпараметр распада для оценок прошлых градиентов  \n",
    "    beta2 -- экспоненциальный гиперпараметр распада для оценок прошлых квадратов градиентов  \n",
    "    epsilon -- гиперпараметр, предотвращающий деление на ноль в обновлениях Adam\n",
    "    num_epochs -- количество эпох\n",
    "    print_cost -- вывод результатов для каждой 1000 эпох\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python словарь содеражщий обновленные параметры \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # количество слоёв в нейронной сети\n",
    "    costs = []                       # для отслеживания стоимости\n",
    "    t = 0                            # инициализация счетчика, необходимого для обновления Adam\n",
    "    seed = 10                        \n",
    "\n",
    "    # Инициализация параметров\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Инициализация оптимизатора\n",
    "    if optimizer == \"gd\":\n",
    "        pass\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Оптимизация\n",
    "    for i in range(num_epochs):        \n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Выбор мини-пакета\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Прямое распространение\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Вычисление потерь\n",
    "            cost = compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Обратное распространение\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Обновление параметров\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam счётчик\n",
    "                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1-Мини-пакетный градиентный спуск\n",
    "\n",
    "Выполните следующий кусок кода, чтобы увидеть, как модель работает с мини-пакетным градиентным спуском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение 3-слойной модели\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "plt.title(\"Model c GD optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.2 - Мини-пакетный градиентный спуск с Momentum\n",
    "\n",
    "Выполните следующий блок кода, чтобы увидеть, как модель работает с Momentum. Поскольку этот пример относительно прост, выигрыш от использования Momentum невелик, но для более сложных задач выигрыш значительно больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение 3-слойной модели\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "plt.title(\"Model c Momentum\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.3 - Мини-пакет с Adam\n",
    "Выполните следующий код, чтобы увидеть, как модель работает с Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение 3-слойной модели\n",
    "layers_dims = [train_X.shape[0], 5, 2, 1]\n",
    "parameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n",
    "\n",
    "# Predict\n",
    "predictions = predict(train_X, train_Y, parameters)\n",
    "\n",
    "plt.title(\"Model с Adam\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.4 - Вывод\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Метод оптимизации**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Потери**\n",
    "        </td>\n",
    "    </tr>\n",
    "        <td>\n",
    "        Gradient descent\n",
    "        </td>\n",
    "        <td>\n",
    "        79.7%\n",
    "        </td>\n",
    "        <td>\n",
    "        Колебаются\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Momentum\n",
    "        </td>\n",
    "        <td>\n",
    "        79.7%\n",
    "        </td>\n",
    "        <td>\n",
    "        Колебаются\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Adam\n",
    "        </td>\n",
    "        <td>\n",
    "        94%\n",
    "        </td>\n",
    "        <td>\n",
    "        Сглажены\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "Momentum обычно помогает в процессе обучении, но учитывая небольшую скорость обучения и упрощенный набор данных, его влияние почти незначительно. Кроме того, огромные колебания, которые вы видите в стоимости, происходят из-за того, что некоторые мини-пакеты сложнее, чем другие, для алгоритма оптимизации.\n",
    "\n",
    "Адам, с другой стороны, явно превосходит мини-пакетный градиентный спуск и Momentum. Если вы запустите модель для нескольких эпох на этом простом наборе данных, все три метода приведут к очень хорошим результатам. Однако вы видели, что Адам сходится гораздо быстрее.\n",
    "\n",
    "Некоторые преимущества Адама включают в себя:\n",
    "- Относительно низкие требования к памяти (хотя и выше, чем градиентный спуск и градиентный спуск с Momentum) \n",
    "- Обычно хорошо работает даже при небольшой настройке гиперпараметров (кроме $\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Ссылки**:\n",
    "\n",
    "- Статья Adam: https://arxiv.org/pdf/1412.6980.pdf"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "Ckiv2",
   "launcher_item_id": "eNLYh"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
