{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточные нейронные сети: Пошаговое руководство\n",
    "\n",
    "В рамках данной лабораторной работы необходимо будет реализовать сверточные (CONV) и пулинговые (POOL) слои с использованием numpy, включая прямое и обратное распространение. \n",
    "\n",
    "**Нотации**:\n",
    "- Надстрочный символ $[l]$ обозначает номер $l^{th}$ слоя. \n",
    "    - Например: $a^{[4]}$ - активация в $4^{th}$ слое. $W^{[5]}$ и $b^{[5]}$ - параметры в $5^{th}$ слое.\n",
    "\n",
    "- Надстрочный символ $(i)$ обозначает $i^{th}$ пример. \n",
    "    - Например: $x^{(i)}$ - $i^{th}$ пример из обучающей выборки.\n",
    "    \n",
    "- Подстрочный символ $i$ обозначает $i^{th}$ номер в векторе.\n",
    "    - Например: $a^{[l]}_i$ - обозначает $i^{th}$ номер активационной функции в слое $l$, если предположить что мы рассматриваем полносвязнный (FC) слой.\n",
    "    \n",
    "- $n_H$, $n_W$ и $n_C$ обозначает соответственно высоту, ширину и количество каналов передаваемых в текущий слой. Если есть необходимость указать конкретный слой $l$, то можно записать следующим образом: $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ и $n_{C_{prev}}$ обозначает высоту, ширину и количество каналов в предыдущем слое. Если есть необходимость указать конкретный слой $l$, то можно записать следующим образом: $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Данный материал опирается и использует материалы курса Deep Learning от организации deeplearning.ai`\n",
    " \n",
    " Ссылка на основной курс (для желающих получить сертификаты): https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Пакеты/Библиотеки\n",
    "\n",
    "Первоначально необходимо запустить ячейку ниже, чтобы импортировать все пакеты, которые вам понадобятся во время лабораторной работы.\n",
    "- [numpy](www.numpy.org) является основным пакетом для научных вычислений в Python.\n",
    "- [matplotlib](http://matplotlib.org) это пакет для отрисовки графиков в Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # размер графиков\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Описание лабораторной работы\n",
    "Задачей в данной лабораторной работе явялется реализовать \"строительные\" блоки сверточной нейронной сети. Каждая функция, которую вы будете реализовывать, детально описана.\n",
    "\n",
    "- Функции по реализации сверточного слоя включают:\n",
    "    - Заполнение нулями\n",
    "    - Окно свёртки\n",
    "    - Convolution forward\n",
    "    - Convolution backward\n",
    "- Функции по реализации пулингово слоя включают:\n",
    "    - Pooling forward\n",
    "    - Создание макси (mask) \n",
    "    - Распределение значений\n",
    "    - Pooling backward (optional)\n",
    "\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**Примечания**: для каждой прямой функции есть соответствующий обратный эквивалент. Следовательно, на каждом шагу модуля прямого распространения необходимо хранить некоторые параметры в кэше. Эти параметры используются для вычисления градиентов во время обратного распространения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Свёрточные нейронные сети\n",
    "\n",
    "Созданные фреймворки помогают быстрее создать сверточный слой, но не помогают понять основы Глубокого обучения. Сверточный слой преобразует входную матрицу в выходную, но уже другого размера.\n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "В данной части необходимо пошагово, с использованием вспомогательных функций, реализовать сверточный слой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding\n",
    "\n",
    "Zero-padding - добавляет рамку вокруг изображения с нулями:\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Рисунок 1** </u><font color='purple'>  : **Zero-Padding**<br> Рисунок (3 канала, RGB) с padding - 2. </center></caption>\n",
    "\n",
    "Ключевые преимущества padding:\n",
    "\n",
    "- Использование слоя CONV без уменьшения размерности по высоте и ширине. Это один из важных аспектов в построении глубокой нейронной сети, так как высота/ширина будет уменьшаться по мере того, как вычисления будет продвигаться к более глубоким слоям. Важным параметром при обучении нейронной сети является - \"same\" свертка, в которой высота/ширина сохраняется после каждого слоя. \n",
    "\n",
    "- Сохраняет больше информации на краях изображения.\n",
    "\n",
    "**Упражнение**: Реализуйте следующую функцию, которая дополняет все изображения из датасета X нулями. [Use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). Обратите внимание, если вы хотите разместить массив \"a\" формы $(5,5,5,5)$ с `pad = 1` для 2-го измерения, `pad = 3` для 4-го измерения и `pad = 0` для остальных, то необходимо сделаеть:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), mode='constant', constant_values = (0,0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Заполнение нулями все изображения из датасета X. Padding применяется к высоте и ширине изображения.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array рамером (m, n_H, n_W, n_C), который содержит в себе m изображений\n",
    "    pad -- целое число, размер заполнения (padding) вокруг каждого изображения по вертикали и горизонтали.\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- результат преобразования (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ### (≈ 1 строка кода)\n",
    "    X_pad = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1,1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход**:\n",
    "\n",
    "```\n",
    "x.shape =\n",
    " (4, 3, 3, 2)\n",
    "x_pad.shape =\n",
    " (4, 7, 7, 2)\n",
    "x[1,1] =\n",
    " [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "x_pad[1,1] =\n",
    " [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Один шаг свёртки\n",
    "\n",
    "В данной части необходимо реализовать один шаг свёртки, в котором применить фильтр к одной позиции входа. Это будет использовано для создания conv слоя, который:\n",
    "\n",
    "- Принимает на вход изображение \n",
    "- Применить фильтр для каждого позиции входного изображения\n",
    "- На выходе возвращает другое изображение (обычно другого размера)\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Рисунок 2** </u><font color='purple'>  : **Операция свёртки**<br>с фильтром 3x3 и шагом (stride) 1 (stride - шаг, с которым вы перемещаете окно каждый раз, когда скользите по изображению.</center></caption>\n",
    "\n",
    "В приложениях компьютерного зрения, каждое значение матрицы слева соответствует одному пикселю, который вычисляется путём свёртки с фильтром 3x3, т.е. поэлементным перемножением каждого значения с исходной матрицией и последующим суммированием их и добавлением смещения. В данной части упражнения необходимо реализовать один шаг свёртки, который соответствует применению фильтра для каждой позиции на изображении.\n",
    "\n",
    "**Упражнение**: Реализуйте conv_single_step(). [Hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заметка**: Переменная b будет передана в виде массива numpy.  Если к массиву numpy добавить скаляр (с плавающей точкой или целое число), то в результате получится массив numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Применение фильтра с параметрами W к срезу (a_slice_prev) выхода с функции активации предыдущего слоя.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- срзе входных данных с рамером (f, f, n_C_prev)\n",
    "    W -- веса (Weight) размером (f, f, n_C_prev)\n",
    "    b -- смещения (Bias) размером (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- скаляр (scalar), результат свёртки скользящего окна с параметрами (W, b) и среза входных данных x    \n",
    "    \"\"\"\n",
    "\n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ### (≈ 2 строки кода)\n",
    "    # Поэлементное умножение a_slice_prev и W. Без добавления b.\n",
    "    s = None\n",
    "    # Суммирование всех записей.\n",
    "    Z = None\n",
    "    # Добавление смещения b к Z. В результате должен получится скаляр.\n",
    "    Z = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -6.99908945068\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Прямое распространение\n",
    "\n",
    "В прямом распространении необъодимо взять много различных фильтров и свернуть их с входом. Каждая \"свертка\" дает на выходе матрицу размером 2D. Затем необходимо сложить выходные данные в стек, чтобы получить размер - 3D: \n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**Упражнение**: \n",
    "Реализуйте функцию свёртки фильтра `W` с входом `A_prev`.  \n",
    "На вход функция принимает:\n",
    "* `A_prev`, выход функции активации из предыдущего слоя (для каждого батча m входов)\n",
    "* Веса (weights) - `W`. Фильтр размером: `f` на `f`.\n",
    "* Смещение (bias) - `b`, где каждый фильтр имеет собственный bias. \n",
    "\n",
    "**Подсказка**: \n",
    "1. Для выбора среза 2х2 в левом верхнем углу матрицы \"a_prev\" (рзмером - (5,5,3)), необходимо сделать:\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "Обратите внимание на пример, в котором высота = 2, ширина = 2 и глубина = 3. Глубина - это количество каналов.  \n",
    "Это будет полезно, когда необходимо определить `a_slice_prev` ниже, используя индексы `start/end`.\n",
    "\n",
    "2. Чтобы определить a_slice, сначала нужно определить ее углы `vert_start`, `vert_end`, `horiz_start` и `horiz_end`. Эта фигура может быть полезна для того, чтобы узнать, как можно определить каждый из углов с помощью h, w, f и s в коде, приведенном ниже.\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Рисунок 3** </u><font color='purple'>  : **Определение среза с помощью вертикального и горизонтального start/end (с 2x2 фильтром)** <br> На этом рисунке показан только один канал.</center></caption>\n",
    "\n",
    "**Напоминание**:\n",
    "Формулы, которые соотносят выходную форму свертки с входной формой::\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{кол-во фильтров используемых для свёртки}$$\n",
    "\n",
    "Для этого упражнения не нужно беспокоиться о векторизации, а просто реализовывать все с помощью for-loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительные подсказки\n",
    "\n",
    "* Вы захотите использовать срезы массивов (например, `varname[0:1,:,3:5]`) для следующих переменных:  \n",
    "  `a_prev_pad`, `W`, `b`.  \n",
    "  Скопируйте стартовый код функции и запустите его вне определенной функции, в отдельных ячейках.  \n",
    "  Проверить, что подмножество каждого массива - это размер и измерение, которое вы ожидаете.  \n",
    "* Чтобы решить, как получить vert_start, vert_end; horiz_start, horiz_end, помните, что это индексы предыдущего слоя. \n",
    "  Нарисуйте пример предыдущего слоя (8 x 8, например) и текущего (выходного) слоя (2 x 2, например).  \n",
    "  Индексы выходного слоя обозначаются `h` и `w`.  \n",
    "* Убедитесь, что `a_slice_prev` имеет высоту, ширину и глубину.\n",
    "* Помните, что `a_prev_pad` является подмножеством `A_prev_pad`.  \n",
    "  Подумайте, какой из них следует использовать в циклах for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Реализуйте прямое распространение для функции свёртки\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- выход функции активации предыдущего слоя, \n",
    "        numpy array размером - (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array размером (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array размером (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary с \"stride\" и \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- результат свёртки, numpy array размером (m, n_H, n_W, n_C)\n",
    "    cache -- cache значений, необходимых для conv_backward() функции\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    # Извлечь размеры из A_prev (≈1 строка кода)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
    "    \n",
    "    # Извлечь размеры из W (≈1 строка кода)\n",
    "    (f, f, n_C_prev, n_C) = None\n",
    "    \n",
    "    # Извлечь информацию из \"hparameters\" (≈2 строки кода)\n",
    "    stride = None\n",
    "    pad = None\n",
    "    \n",
    "    # Вычислите размеры выходного объема CONV по формуле, приведенной выше. (≈2 строки кода)\n",
    "    # Подсказка: используйте int() операции округления. \n",
    "    n_H = None\n",
    "    n_W = None\n",
    "    \n",
    "    # Инициализируйте выходной значение Z нулями. (≈1 строка кода)\n",
    "    Z = None\n",
    "    \n",
    "    # создайте A_prev_pad путём заполнения A_prev\n",
    "    A_prev_pad = None\n",
    "    \n",
    "    for i in range(None):               # цикл по всем побучающим примерам\n",
    "        a_prev_pad = None               # выбор i-го обучающего примера\n",
    "        for h in range(None):           # цикл по вертикальной оси выходного параметра\n",
    "            # Найти вертикальное начало и конец текущего \"среза\". (≈2 строки кода)\n",
    "            vert_start = None\n",
    "            vert_end = None\n",
    "            \n",
    "            for w in range(None):       # lикл по горизонтальной оси выходного параметра\n",
    "                # Найти горизонтальное начало и конец текущего \"среза\"(≈2 строки кода)\n",
    "                horiz_start = None\n",
    "                horiz_end = None\n",
    "                \n",
    "                for c in range(None):   # цикл по всем каналам (= #filters)\n",
    "                                        \n",
    "                    # Используйте углы, чтобы определить срез a_prev_pad. (≈1 строка кода)\n",
    "                    a_slice_prev = None\n",
    "                    \n",
    "                    # Свёртка с корректным фильтром W и смещением b. (≈3 строка кода)\n",
    "                    weights = None\n",
    "                    biases = None\n",
    "                    Z[i, h, w, c] = None\n",
    "                                        \n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    # Проверка на то, что выходная форма правильная\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Сохранение информации в \"cache\" для обратного распространения ошибки.\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4)\n",
    "W = np.random.randn(3,3,4,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход**:\n",
    "```\n",
    "Z's mean =\n",
    " 0.692360880758\n",
    "Z[3,2,1] =\n",
    " [ -1.28912231   2.27650251   6.61941931   0.95527176   8.25132576\n",
    "   2.31329639  13.00689405   2.34576051]\n",
    "cache_conv[0][1][2][3] = [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, слой CONV также должен содержать активацию, в этом случае добавлена следующая строка кода:\n",
    "\n",
    "```python\n",
    "# Получение результатов свёртки\n",
    "Z[i, h, w, c] = ...\n",
    "# Применение активации\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Пулинговый (Pooling) слой \n",
    "\n",
    "Пулинговый (POOL) слой уменьшает высоту и ширину входа. Это помогает уменьшить количество вычислений, а также помогает сделать детекторы более инвариантными к положению объекта на входе:\n",
    "\n",
    "- Max-pooling слой: ($f, f$) выбирается максимальное значение.\n",
    "\n",
    "- Average-pooling layer: ($f, f$) выбирается среднее значение.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "Пулинговые слои не имеют параметров для обратного распространения ошибки. Но они имеют гиперпараметры, такие  как размер окна $f$. Который определяет высоту и ширину $f \\times f$ окна для вычисления *max* или *average*. \n",
    "\n",
    "### 4.1 - Pooling для прямого распространения\n",
    "Далее необходимо реализовать MAX-POOL и AVG-POOL, в одной и той же функции. \n",
    "\n",
    "**Упражнение**: Реализуйте прямой проход. Следуйте указаниям в комментариях ниже.\n",
    "\n",
    "**Напоминание**:\n",
    "Вычисление выходного размера:\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЦЕНИВАЕМОЕ: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Реализация прямого распространения для пулингового слоя\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- входные данные, numpy array размером (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary, который содержит \"f\" и \"stride\"\n",
    "    mode -- тип пулинга (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- выходное значение pool слоя, a numpy array размером (m, n_H, n_W, n_C)\n",
    "    cache -- кэш, используемый в обратном проходе слоя пула, который содержит входы и гиперпараметры  \n",
    "    \"\"\"\n",
    "    \n",
    "    # Входной размер\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Извлечение гиперпараметров \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Определите размеры выходного слоя\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Инициализация выходной матрицы A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    for i in range(None):                         # цико по обучающим примерам\n",
    "        for h in range(None):                     # цикл по вертикальной оси выходного параметра\n",
    "            vert_start = None\n",
    "            vert_end = None\n",
    "            \n",
    "            for w in range(None):                 # цикл по горизонтальной оси выходного параметра\n",
    "                horiz_start = None\n",
    "                horiz_end = None\n",
    "                \n",
    "                for c in range (None):            # цикл по каналам выходного параметра\n",
    "                    \n",
    "                    # Используйте углы, чтобы определить текущий срез на i-м обучающем примере A_prev, канал c. (≈1 строка)\n",
    "                    a_prev_slice = None\n",
    "                    \n",
    "                    # Провести вычисление операции объединения.\n",
    "                    # Используйте np.max и np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = None\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = None\n",
    "    \n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    # Сохранение параметров в \"cache\" для pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Проверка корректности размерности\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход**\n",
    "```\n",
    "mode = max\n",
    "A.shape = (2, 3, 3, 3)\n",
    "A =\n",
    " [[[[ 1.74481176  0.90159072  1.65980218]\n",
    "   [ 1.74481176  1.46210794  1.65980218]\n",
    "   [ 1.74481176  1.6924546   1.65980218]]\n",
    "\n",
    "  [[ 1.14472371  0.90159072  2.10025514]\n",
    "   [ 1.14472371  0.90159072  1.65980218]\n",
    "   [ 1.14472371  1.6924546   1.65980218]]\n",
    "\n",
    "  [[ 1.13162939  1.51981682  2.18557541]\n",
    "   [ 1.13162939  1.51981682  2.18557541]\n",
    "   [ 1.13162939  1.6924546   2.18557541]]]\n",
    "\n",
    "\n",
    " [[[ 1.19891788  0.84616065  0.82797464]\n",
    "   [ 0.69803203  0.84616065  1.2245077 ]\n",
    "   [ 0.69803203  1.12141771  1.2245077 ]]\n",
    "\n",
    "  [[ 1.96710175  0.84616065  1.27375593]\n",
    "   [ 1.96710175  0.84616065  1.23616403]\n",
    "   [ 1.62765075  1.12141771  1.2245077 ]]\n",
    "\n",
    "  [[ 1.96710175  0.86888616  1.27375593]\n",
    "   [ 1.96710175  0.86888616  1.23616403]\n",
    "   [ 1.62765075  1.12141771  0.79280687]]]]\n",
    "\n",
    "mode = average\n",
    "A.shape = (2, 3, 3, 3)\n",
    "A =\n",
    " [[[[ -3.01046719e-02  -3.24021315e-03  -3.36298859e-01]\n",
    "   [  1.43310483e-01   1.93146751e-01  -4.44905196e-01]\n",
    "   [  1.28934436e-01   2.22428468e-01   1.25067597e-01]]\n",
    "\n",
    "  [[ -3.81801899e-01   1.59993515e-02   1.70562706e-01]\n",
    "   [  4.73707165e-02   2.59244658e-02   9.20338402e-02]\n",
    "   [  3.97048605e-02   1.57189094e-01   3.45302489e-01]]\n",
    "\n",
    "  [[ -3.82680519e-01   2.32579951e-01   6.25997903e-01]\n",
    "   [ -2.47157416e-01  -3.48524998e-04   3.50539717e-01]\n",
    "   [ -9.52551510e-02   2.68511000e-01   4.66056368e-01]]]\n",
    "\n",
    "\n",
    " [[[ -1.73134159e-01   3.23771981e-01  -3.43175716e-01]\n",
    "   [  3.80634669e-02   7.26706274e-02  -2.30268958e-01]\n",
    "   [  2.03009393e-02   1.41414785e-01  -1.23158476e-02]]\n",
    "\n",
    "  [[  4.44976963e-01  -2.61694592e-03  -3.10403073e-01]\n",
    "   [  5.08114737e-01  -2.34937338e-01  -2.39611830e-01]\n",
    "   [  1.18726772e-01   1.72552294e-01  -2.21121966e-01]]\n",
    "\n",
    "  [[  4.29449255e-01   8.44699612e-02  -2.72909051e-01]\n",
    "   [  6.76351685e-01  -1.20138225e-01  -2.44076712e-01]\n",
    "   [  1.50774518e-01   2.89111751e-01   1.23238536e-03]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход:**\n",
    "    \n",
    "```\n",
    "mode = max\n",
    "A.shape = (2, 2, 2, 3)\n",
    "A =\n",
    " [[[[ 1.74481176  0.90159072  1.65980218]\n",
    "   [ 1.74481176  1.6924546   1.65980218]]\n",
    "\n",
    "  [[ 1.13162939  1.51981682  2.18557541]\n",
    "   [ 1.13162939  1.6924546   2.18557541]]]\n",
    "\n",
    "\n",
    " [[[ 1.19891788  0.84616065  0.82797464]\n",
    "   [ 0.69803203  1.12141771  1.2245077 ]]\n",
    "\n",
    "  [[ 1.96710175  0.86888616  1.27375593]\n",
    "   [ 1.62765075  1.12141771  0.79280687]]]]\n",
    "\n",
    "mode = average\n",
    "A.shape = (2, 2, 2, 3)\n",
    "A =\n",
    " [[[[-0.03010467 -0.00324021 -0.33629886]\n",
    "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
    "\n",
    "  [[-0.38268052  0.23257995  0.6259979 ]\n",
    "   [-0.09525515  0.268511    0.46605637]]]\n",
    "\n",
    "\n",
    " [[[-0.17313416  0.32377198 -0.34317572]\n",
    "   [ 0.02030094  0.14141479 -0.01231585]]\n",
    "\n",
    "  [[ 0.42944926  0.08446996 -0.27290905]\n",
    "   [ 0.15077452  0.28911175  0.00123239]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Обратное распространение по нейронной сети\n",
    "\n",
    "\n",
    "В современных фреймоворках глубокого обучения достаточно реализовать прямой проход, а фреймоворки заботятся о обратном проходе, поэтому большинству инженеров-специалистов в области глубокого обучения не нужно беспокоиться о деталях обратного прохода. Обратный проход для свёрточных сетей сложен.  \n",
    "\n",
    "Когда в предыдущем курсе вы реализовали простую (полностью подключенную) нейронную сеть, вы использовали обратное распространения для вычисления производных относительно функции потерь. Аналогично, в свёрточных нейронных сетях можно вычислить производные относительно функции потерь для обновления параметров.\n",
    "\n",
    "### 5.1 - Свёртка для обратного распространения\n",
    "\n",
    "#### 5.1.1 - Вычисление dA:\n",
    "Это формула вычисления $dA$ относительно стоимости определенного фильтра $W_c$:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "где $W_c$ - фильтр, а $dZ_{hw}$ - скаляр, соответствующий градиенту стоимости относительно вывода слоя Z в h-й строке и w-му столбце (соответствует точечному произведению, взятому при i-м шаге влево и j-м шаге вниз). Обратите внимание, что каждый раз при обновлении dA мы умножаем один и тот же фильтр $W_c$ на разные dZ. Мы делаем это в основном потому, что при вычислении прямого распространения, каждый фильтр точечно суммируется разными a_slice. Поэтому при вычислении обратного распространения для dA мы просто добавляем градиенты всех a_slices.  \n",
    "\n",
    "В коде, внутри соответствующих for-loops:\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - Вычисление dW:\n",
    "Это формула вычисляет $dW_c$ ($dW_c$ - производная одного фильтра) относительно функции потерь:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "где $a_{slice}$ соответствует срезу, который использовался для активации $Z_{ij}$. Таким образом, получается градиент для $W$ по отношению к этому срезу. Поскольку это те же самые $W$, необходимо сложить все такие градиенты, чтобы получить $dW$.\n",
    "\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - Вычисление db:\n",
    "\n",
    "Это формула вычисляет $db$ в отношении потерь для определённого фильтра $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "В базовых нейронных сетях, db вычисляется суммированием $dZ$. В этом случае просто суммируются все градиенты вывода conv (Z) относительно стоимости. \n",
    "\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**Упражнение**: Реализуйте `conv_backward`. Необходимо суммировать все примеры обучения, фильтры, высоту и ширину. Затем следует вычислить производные по формулам 1, 2 и 3, приведенным выше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространения для свёртки\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- градиент по потерям относительно вывода слоя conv (Z), array numpy разер (m, n_H, n_W, n_C)\n",
    "    cache -- cache значений для вычисления conv_backward(), выход функции conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- градиент стоимости относительно ввода слоя свертки (A_prev),\n",
    "               numpy array размером (m, n_H_prev, n_W_prev, n_C_prev)                \n",
    "    dW -- градиент стоимости относительно веса conv слоя (W)\n",
    "          numpy array размером (f, f, n_C_prev, n_C)\n",
    "    db -- градиент стоимости относительно biases по conv слою (b)\n",
    "          numpy array размером (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    # Извлечение \"cache\"\n",
    "    (A_prev, W, b, hparameters) = None\n",
    "    \n",
    "    # Извлечение размера из A_prev\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = None\n",
    "    \n",
    "    # Извлечение размера из W\n",
    "    (f, f, n_C_prev, n_C) = None\n",
    "    \n",
    "    # Извлечение информации из \"hparameters\"\n",
    "    stride = None\n",
    "    pad = None\n",
    "    \n",
    "    # Извлечение размера из dZ\n",
    "    (m, n_H, n_W, n_C) = None\n",
    "    \n",
    "    # Иницализация dA_prev, dW, db с корректными размерами\n",
    "    dA_prev = None                           \n",
    "    dW = None\n",
    "    db = None\n",
    "\n",
    "    # Заполнение A_prev и dA_prev\n",
    "    A_prev_pad = None\n",
    "    dA_prev_pad = None\n",
    "    \n",
    "    for i in range(None):                       # цикл по обучающим примерам\n",
    "        \n",
    "        # выбор i-го обучающего примера из A_prev_pad и dA_prev_pad\n",
    "        a_prev_pad = None\n",
    "        da_prev_pad = None\n",
    "        \n",
    "        for h in range(None):                   # цикл по вертикальной оси\n",
    "            for w in range(None):               # цикл по горизонтальной оси\n",
    "                for c in range(None):           # цикл по каналам                    \n",
    "                    vert_start = None\n",
    "                    vert_end = None\n",
    "                    horiz_start = None\n",
    "                    horiz_end = None\n",
    "                    \n",
    "                    # Используйте углы, чтобы определить срез из a_prev_pad.\n",
    "                    a_slice = None\n",
    "\n",
    "                    # Обновление градиентов для параметров фильтра с помощью приведенных выше формул\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += None\n",
    "                    dW[:,:,:,c] += None\n",
    "                    db[:,:,:,c] += None\n",
    "                    \n",
    "        # Установить i-й обучающий пример dA_prev в недобавленный da_prev_pad (Подсказка: X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    # Проверка корректности размера\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Тестирование conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход:**\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dA_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.45243777754\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dW_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.72699145831\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **db_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            7.83923256462\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pooling слой - обратное распространение\n",
    "\n",
    "Далее, необходимо реализовать обратный проход для pooling-слоя, начиная со слоя MAX-POOL. Даже несмотря на то, что pooling слой не имеет параметров для обновления backprop, все равно нужно выполнить обратный переход через pooling слой, чтобы вычислить градиенты для слоев, которые пришли до pooling слоя. \n",
    "\n",
    "### 5.2.1 Max pooling - обратное распространение\n",
    "\n",
    "Перед тем, как перейти к обратной разметке pooling, необходимо построить вспомогательную функцию `create_mask_from_window()`, которая делает следующее: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "Как видно, эта функция создает матрицу \"mask\", которая отслеживает, где находится максимум матрицы. True (1) указывает на позицию максимума в X, остальные записи - False (0). \n",
    "\n",
    "**Упражнение**: Реализуйте `create_mask_from_window()`. \n",
    "Подсказки:\n",
    "- [np.max()]() - вычисляет максимум в массиве.\n",
    "- Если у вас есть матрица X и скаляр x: `A = (X == x)` вернет матрицу А того же размера, что и X:\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- Здесь не нужно рассматривать случаи, когда в матрице есть несколько максимумов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Создает маску из входной матрицы x, чтобы определить максимальный вход x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array размером (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask --  Array той же формы, что и окно, содержит True в позиции, соответствующей максимальному вводу x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ### (≈1 строка кода)\n",
    "    mask = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход:** \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**x =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "[[ 1.62434536 -0.61175641 -0.52817175] <br>\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**mask =**\n",
    "</td>\n",
    "<td>\n",
    "[[ True False False] <br>\n",
    " [False False False]]\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we keep track of the position of the max? It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Average pooling - обратное распространение\n",
    "\n",
    "При max pooling, для каждого окна, все \"влияние\" на выход поступало от одного входного значения - максимум. В average pooling каждый элемент окна входа имеет равное влияние на выход. Поэтому для реализации backprop необходимо реализовать вспомогательную функцию, которая отражает это.\n",
    "\n",
    "Например, если сделать average pooling в прямом проходе с помощью фильтра 2x2, то маска, которую необходимо использовать для обратного прохода, будет выглядеть так: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "Это означает, что каждая позиция в матрице $dZ$ одинаково способствует выходу, потому что в прямом проходе использовалось среднее. \n",
    "\n",
    "**Упражнение**: Выполните функцию, описанную ниже, чтобы равномерно распределить значение dz по матрице нужной формы. [Подсказка](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Распределяет входное значение в матрице до необходимо размера\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- входной скаляр\n",
    "    shape -- размер (n_H, n_W) выходной матрицы, для которой распределяется значение dz.\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array размером (n_H, n_W) для распределено значение dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    # Извлечение размеров (≈1 строка кода)\n",
    "    (n_H, n_W) = None\n",
    "    \n",
    "    # Вычислить значение для распределения по матрице (≈1 строка кода)\n",
    "    average = None\n",
    "    \n",
    "    # Создать матрицу, в которой каждая запись будет представлять собой \"average\" значение. (≈1 строка кода)\n",
    "    a = None\n",
    "    ### ОКОНЧАНИЕ ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход**: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "distributed_value =\n",
    "</td>\n",
    "<td>\n",
    "[[ 0.5  0.5]\n",
    "<br\\> \n",
    "[ 0.5  0.5]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Складываем вместе: Pooling обратное распространение\n",
    "\n",
    "Теперь у вас есть все, что нужно для расчета обратного распространения на уровне пула.\n",
    "\n",
    "**Упражнение**: \n",
    "Реализуйте функцию `pool_backward` в обоих режимах (`max\"` и `average\"`). Необходимо использовать 4 for-loops (итерации по тренировочным примерам, высоте, ширине и каналам). Необходимо использовать оператор `if/elif`, чтобы увидеть, равен ли режим `'max'` или `'average'`. Если она равна 'average', необходимо использовать функцию `distribute_value()`, которая реализована выше, для создания матрицы той же формы, что и `a_slice`. В противном случае режим будет равен '`max`', и необходимо создадите маску с `create_mask_from_window()` и умножить ее на соответствующее значение dA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Реализация обратного распространения для пулингового слоя\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- градиент стоимости относительно выхода слоя-pool, такая же форма, как и у A\n",
    "    cache -- cache вывод из прямого прохода слоя-pool, содержит входные значения слоя и hparameters     \n",
    "    mode -- \"max\" или \"average\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- градиент стоимости по отношению к pooling, та же форма, что и A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### НАЧАЛО ВАШЕГО КОДА ЗДЕСЬ ###\n",
    "    \n",
    "    # Извлечение параметров cache (≈1 строка кода)\n",
    "    (A_prev, hparameters) = None\n",
    "    \n",
    "    # Извлечение гиперпараметров из \"hparameters\" (≈2 строки кода)\n",
    "    stride = None\n",
    "    f = None\n",
    "    \n",
    "    # Размер A_prev и dA (≈2 строки кода)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = None\n",
    "    m, n_H, n_W, n_C = None\n",
    "    \n",
    "    # Инициализация dA_prev нулями (≈1 строка кода)\n",
    "    dA_prev = None\n",
    "    \n",
    "    for i in range(None):                       # цикл по обучающим примерам\n",
    "        \n",
    "        # Выбор обучающего примера из A_prev (≈1 строка кода)\n",
    "        a_prev = None\n",
    "        \n",
    "        for h in range(None):                   # цикл по вертикальной оси\n",
    "            for w in range(None):               # цикл по горизонтальной оси\n",
    "                for c in range(None):           # цикл по каналам\n",
    "                    vert_start = None\n",
    "                    vert_end = None\n",
    "                    horiz_start = None\n",
    "                    horiz_end = None                    \n",
    "                    if mode == \"max\":\n",
    "                        # Используйте углы и \"c\", чтобы определить текущий срез из a_prev. (≈1 строка кода)\n",
    "                        a_prev_slice = None\n",
    "                        # Создайте mask из a_prev_slice (≈1 строка кода)\n",
    "                        mask = None\n",
    "                        # Установите dA_prev = dA_prev + (маска, умноженная на правильную запись dA) (≈1 строка кода)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += None\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Вычисление dA (≈1 строка кода)\n",
    "                        da = None\n",
    "                        # Размер фильтра fxf (≈1 строка кода)\n",
    "                        shape = None\n",
    "                        # Добавление da с правильным срезом к dA_prev (≈1 строка кода)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += None\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Проверка размера\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый выход**: \n",
    "\n",
    "mode = max:\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.          0.        ] <br>\n",
    " [ 5.05844394 -1.68282702] <br>\n",
    " [ 0.          0.        ]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "mode = average\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.08485462  0.2787552 ] <br>\n",
    " [ 1.26461098 -0.25749373] <br>\n",
    " [ 1.17975636 -0.53624893]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
